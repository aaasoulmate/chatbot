{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 234,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 3.062490701675415,
      "learning_rate": 0.0004999970800043822,
      "loss": 2.3277,
      "step": 1
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 0.8911314010620117,
      "learning_rate": 0.0004999883200857396,
      "loss": 2.2973,
      "step": 2
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 0.875841498374939,
      "learning_rate": 0.000499973720448704,
      "loss": 2.2863,
      "step": 3
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 0.8000545501708984,
      "learning_rate": 0.0004999532814343218,
      "loss": 2.0042,
      "step": 4
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 1.0447192192077637,
      "learning_rate": 0.0004999270035200482,
      "loss": 2.1547,
      "step": 5
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 0.8461388945579529,
      "learning_rate": 0.0004998948873197343,
      "loss": 1.7036,
      "step": 6
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 0.953872799873352,
      "learning_rate": 0.000499856933583613,
      "loss": 2.1819,
      "step": 7
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 0.8914564847946167,
      "learning_rate": 0.0004998131431982825,
      "loss": 1.9024,
      "step": 8
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 1.0784956216812134,
      "learning_rate": 0.0004997635171866848,
      "loss": 2.5142,
      "step": 9
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 0.8521708846092224,
      "learning_rate": 0.0004997080567080817,
      "loss": 2.3314,
      "step": 10
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 0.8128117918968201,
      "learning_rate": 0.000499646763058028,
      "loss": 1.9351,
      "step": 11
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 0.6701896786689758,
      "learning_rate": 0.000499579637668341,
      "loss": 1.713,
      "step": 12
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9138104915618896,
      "learning_rate": 0.0004995066821070679,
      "loss": 2.039,
      "step": 13
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 0.9113458395004272,
      "learning_rate": 0.0004994278980784478,
      "loss": 1.4375,
      "step": 14
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 0.9478543400764465,
      "learning_rate": 0.0004993432874228728,
      "loss": 1.9903,
      "step": 15
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.7804792523384094,
      "learning_rate": 0.0004992528521168449,
      "loss": 1.4921,
      "step": 16
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 1.0119937658309937,
      "learning_rate": 0.0004991565942729299,
      "loss": 1.8789,
      "step": 17
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 0.8728306293487549,
      "learning_rate": 0.0004990545161397072,
      "loss": 2.1199,
      "step": 18
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 0.7694759964942932,
      "learning_rate": 0.0004989466201017188,
      "loss": 1.676,
      "step": 19
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 0.7507703900337219,
      "learning_rate": 0.0004988329086794121,
      "loss": 1.4953,
      "step": 20
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 0.7191950082778931,
      "learning_rate": 0.0004987133845290822,
      "loss": 1.6628,
      "step": 21
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 0.6677369475364685,
      "learning_rate": 0.000498588050442809,
      "loss": 1.7452,
      "step": 22
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 0.8734827637672424,
      "learning_rate": 0.0004984569093483921,
      "loss": 1.8428,
      "step": 23
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.7459938526153564,
      "learning_rate": 0.0004983199643092832,
      "loss": 1.5512,
      "step": 24
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 0.9081287980079651,
      "learning_rate": 0.0004981772185245135,
      "loss": 1.7426,
      "step": 25
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6891407370567322,
      "learning_rate": 0.0004980286753286195,
      "loss": 1.3501,
      "step": 26
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 0.8247389197349548,
      "learning_rate": 0.000497874338191565,
      "loss": 2.0243,
      "step": 27
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 0.570045530796051,
      "learning_rate": 0.0004977142107186602,
      "loss": 0.9634,
      "step": 28
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 0.7795639634132385,
      "learning_rate": 0.0004975482966504772,
      "loss": 1.8785,
      "step": 29
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 0.9852104783058167,
      "learning_rate": 0.0004973765998627628,
      "loss": 1.6077,
      "step": 30
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 0.6977394819259644,
      "learning_rate": 0.000497199124366348,
      "loss": 1.2593,
      "step": 31
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 0.9683637022972107,
      "learning_rate": 0.0004970158743070541,
      "loss": 1.492,
      "step": 32
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 0.86052405834198,
      "learning_rate": 0.0004968268539655963,
      "loss": 1.4458,
      "step": 33
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.8665276765823364,
      "learning_rate": 0.0004966320677574827,
      "loss": 1.4314,
      "step": 34
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 0.8010600805282593,
      "learning_rate": 0.0004964315202329127,
      "loss": 1.8197,
      "step": 35
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 1.1157814264297485,
      "learning_rate": 0.0004962252160766693,
      "loss": 1.9557,
      "step": 36
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 0.8435461521148682,
      "learning_rate": 0.0004960131601080103,
      "loss": 1.3016,
      "step": 37
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 0.901960551738739,
      "learning_rate": 0.0004957953572805558,
      "loss": 1.6526,
      "step": 38
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.778903603553772,
      "learning_rate": 0.0004955718126821722,
      "loss": 1.0099,
      "step": 39
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 0.9405866861343384,
      "learning_rate": 0.0004953425315348534,
      "loss": 1.3401,
      "step": 40
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 0.888658881187439,
      "learning_rate": 0.0004951075191945989,
      "loss": 1.1657,
      "step": 41
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.829555094242096,
      "learning_rate": 0.000494866781151289,
      "loss": 1.0982,
      "step": 42
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 0.8553903102874756,
      "learning_rate": 0.0004946203230285557,
      "loss": 0.9545,
      "step": 43
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 0.9111601114273071,
      "learning_rate": 0.0004943681505836522,
      "loss": 1.2767,
      "step": 44
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 1.0618258714675903,
      "learning_rate": 0.0004941102697073181,
      "loss": 1.4246,
      "step": 45
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 0.8991016149520874,
      "learning_rate": 0.0004938466864236413,
      "loss": 0.9724,
      "step": 46
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 1.0322668552398682,
      "learning_rate": 0.0004935774068899184,
      "loss": 1.4172,
      "step": 47
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 0.9276526570320129,
      "learning_rate": 0.0004933024373965097,
      "loss": 1.1873,
      "step": 48
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 0.8406540155410767,
      "learning_rate": 0.0004930217843666929,
      "loss": 1.1714,
      "step": 49
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.9606499671936035,
      "learning_rate": 0.000492735454356513,
      "loss": 1.0526,
      "step": 50
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 0.9613040685653687,
      "learning_rate": 0.000492443454054629,
      "loss": 1.0692,
      "step": 51
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1655267477035522,
      "learning_rate": 0.0004921457902821578,
      "loss": 1.4181,
      "step": 52
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 0.9515112042427063,
      "learning_rate": 0.0004918424699925146,
      "loss": 0.9476,
      "step": 53
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 1.0348467826843262,
      "learning_rate": 0.0004915335002712506,
      "loss": 1.2883,
      "step": 54
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 1.1461851596832275,
      "learning_rate": 0.0004912188883358879,
      "loss": 1.0417,
      "step": 55
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 1.3489856719970703,
      "learning_rate": 0.0004908986415357501,
      "loss": 0.8954,
      "step": 56
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 1.0444974899291992,
      "learning_rate": 0.0004905727673517914,
      "loss": 0.7511,
      "step": 57
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 1.2593330144882202,
      "learning_rate": 0.0004902412733964212,
      "loss": 0.7648,
      "step": 58
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 1.1966091394424438,
      "learning_rate": 0.0004899041674133265,
      "loss": 0.7959,
      "step": 59
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 1.2979122400283813,
      "learning_rate": 0.0004895614572772916,
      "loss": 0.7444,
      "step": 60
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 1.0687201023101807,
      "learning_rate": 0.0004892131509940129,
      "loss": 0.6503,
      "step": 61
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 1.1869425773620605,
      "learning_rate": 0.0004888592566999134,
      "loss": 0.5829,
      "step": 62
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 1.3409245014190674,
      "learning_rate": 0.0004884997826619511,
      "loss": 0.7492,
      "step": 63
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 2.0238184928894043,
      "learning_rate": 0.000488134737277427,
      "loss": 1.2244,
      "step": 64
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.66524076461792,
      "learning_rate": 0.0004877641290737884,
      "loss": 0.7633,
      "step": 65
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 1.128322958946228,
      "learning_rate": 0.00048738796670843004,
      "loss": 0.5971,
      "step": 66
    },
    {
      "epoch": 2.5769230769230766,
      "grad_norm": 1.5947389602661133,
      "learning_rate": 0.0004870062589684916,
      "loss": 1.0266,
      "step": 67
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 1.2906712293624878,
      "learning_rate": 0.00048661901477065246,
      "loss": 0.6898,
      "step": 68
    },
    {
      "epoch": 2.6538461538461537,
      "grad_norm": 1.4268180131912231,
      "learning_rate": 0.0004862262431609235,
      "loss": 0.76,
      "step": 69
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 1.1682125329971313,
      "learning_rate": 0.00048582795331443573,
      "loss": 0.697,
      "step": 70
    },
    {
      "epoch": 2.730769230769231,
      "grad_norm": 1.2468115091323853,
      "learning_rate": 0.0004854241545352262,
      "loss": 0.6049,
      "step": 71
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 1.6212148666381836,
      "learning_rate": 0.00048501485625601995,
      "loss": 1.0165,
      "step": 72
    },
    {
      "epoch": 2.8076923076923075,
      "grad_norm": 1.2294979095458984,
      "learning_rate": 0.00048460006803801057,
      "loss": 0.7809,
      "step": 73
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 1.5046968460083008,
      "learning_rate": 0.00048417979957063626,
      "loss": 0.9687,
      "step": 74
    },
    {
      "epoch": 2.8846153846153846,
      "grad_norm": 1.2751010656356812,
      "learning_rate": 0.00048375406067135377,
      "loss": 0.5697,
      "step": 75
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 1.437254786491394,
      "learning_rate": 0.0004833228612854087,
      "loss": 0.9493,
      "step": 76
    },
    {
      "epoch": 2.9615384615384617,
      "grad_norm": 1.149757742881775,
      "learning_rate": 0.00048288621148560374,
      "loss": 0.6825,
      "step": 77
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.5624420642852783,
      "learning_rate": 0.00048244412147206283,
      "loss": 0.6551,
      "step": 78
    },
    {
      "epoch": 3.0384615384615383,
      "grad_norm": 1.3018054962158203,
      "learning_rate": 0.0004819966015719933,
      "loss": 0.3913,
      "step": 79
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 1.5454065799713135,
      "learning_rate": 0.00048154366223944413,
      "loss": 0.5638,
      "step": 80
    },
    {
      "epoch": 3.1153846153846154,
      "grad_norm": 0.9651458859443665,
      "learning_rate": 0.0004810853140550624,
      "loss": 0.2996,
      "step": 81
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 1.918836236000061,
      "learning_rate": 0.0004806215677258456,
      "loss": 0.4722,
      "step": 82
    },
    {
      "epoch": 3.1923076923076925,
      "grad_norm": 3.3249425888061523,
      "learning_rate": 0.0004801524340848917,
      "loss": 0.5774,
      "step": 83
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 1.6645357608795166,
      "learning_rate": 0.0004796779240911461,
      "loss": 0.309,
      "step": 84
    },
    {
      "epoch": 3.269230769230769,
      "grad_norm": 1.801596760749817,
      "learning_rate": 0.00047919804882914566,
      "loss": 0.3199,
      "step": 85
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 1.2561839818954468,
      "learning_rate": 0.0004787128195087596,
      "loss": 0.1961,
      "step": 86
    },
    {
      "epoch": 3.3461538461538463,
      "grad_norm": 1.696256399154663,
      "learning_rate": 0.0004782222474649279,
      "loss": 0.3921,
      "step": 87
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 1.709801197052002,
      "learning_rate": 0.00047772634415739625,
      "loss": 0.3262,
      "step": 88
    },
    {
      "epoch": 3.423076923076923,
      "grad_norm": 1.4217872619628906,
      "learning_rate": 0.00047722512117044863,
      "loss": 0.264,
      "step": 89
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 1.4318925142288208,
      "learning_rate": 0.00047671859021263636,
      "loss": 0.3307,
      "step": 90
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.3107478618621826,
      "learning_rate": 0.0004762067631165049,
      "loss": 0.3257,
      "step": 91
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 1.5463006496429443,
      "learning_rate": 0.00047568965183831727,
      "loss": 0.4498,
      "step": 92
    },
    {
      "epoch": 3.5769230769230766,
      "grad_norm": 1.6494580507278442,
      "learning_rate": 0.0004751672684577747,
      "loss": 0.3226,
      "step": 93
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 1.367875337600708,
      "learning_rate": 0.00047463962517773473,
      "loss": 0.2571,
      "step": 94
    },
    {
      "epoch": 3.6538461538461537,
      "grad_norm": 1.487399697303772,
      "learning_rate": 0.0004741067343239259,
      "loss": 0.3475,
      "step": 95
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 1.2220972776412964,
      "learning_rate": 0.0004735686083446599,
      "loss": 0.274,
      "step": 96
    },
    {
      "epoch": 3.730769230769231,
      "grad_norm": 1.1666843891143799,
      "learning_rate": 0.00047302525981054067,
      "loss": 0.3128,
      "step": 97
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 1.6472101211547852,
      "learning_rate": 0.000472476701414171,
      "loss": 0.5345,
      "step": 98
    },
    {
      "epoch": 3.8076923076923075,
      "grad_norm": 1.2984520196914673,
      "learning_rate": 0.0004719229459698556,
      "loss": 0.2678,
      "step": 99
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 1.9854828119277954,
      "learning_rate": 0.00047136400641330245,
      "loss": 0.7538,
      "step": 100
    },
    {
      "epoch": 3.8846153846153846,
      "grad_norm": 1.6915010213851929,
      "learning_rate": 0.00047079989580132,
      "loss": 0.5318,
      "step": 101
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 1.5479490756988525,
      "learning_rate": 0.00047023062731151217,
      "loss": 0.3552,
      "step": 102
    },
    {
      "epoch": 3.9615384615384617,
      "grad_norm": 0.9466544389724731,
      "learning_rate": 0.00046965621424197116,
      "loss": 0.2351,
      "step": 103
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.953060269355774,
      "learning_rate": 0.0004690766700109659,
      "loss": 0.582,
      "step": 104
    },
    {
      "epoch": 4.038461538461538,
      "grad_norm": 0.9763156771659851,
      "learning_rate": 0.0004684920081566295,
      "loss": 0.1589,
      "step": 105
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 1.1878637075424194,
      "learning_rate": 0.00046790224233664236,
      "loss": 0.1381,
      "step": 106
    },
    {
      "epoch": 4.115384615384615,
      "grad_norm": 1.0606249570846558,
      "learning_rate": 0.00046730738632791327,
      "loss": 0.1387,
      "step": 107
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 1.2649343013763428,
      "learning_rate": 0.0004667074540262577,
      "loss": 0.1089,
      "step": 108
    },
    {
      "epoch": 4.1923076923076925,
      "grad_norm": 1.0589369535446167,
      "learning_rate": 0.00046610245944607334,
      "loss": 0.0815,
      "step": 109
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 2.0785624980926514,
      "learning_rate": 0.0004654924167200123,
      "loss": 0.1566,
      "step": 110
    },
    {
      "epoch": 4.269230769230769,
      "grad_norm": 1.4229745864868164,
      "learning_rate": 0.00046487734009865127,
      "loss": 0.1276,
      "step": 111
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 1.3154816627502441,
      "learning_rate": 0.0004642572439501586,
      "loss": 0.1527,
      "step": 112
    },
    {
      "epoch": 4.346153846153846,
      "grad_norm": 1.0467482805252075,
      "learning_rate": 0.00046363214275995854,
      "loss": 0.0854,
      "step": 113
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 0.9731992483139038,
      "learning_rate": 0.00046300205113039295,
      "loss": 0.1006,
      "step": 114
    },
    {
      "epoch": 4.423076923076923,
      "grad_norm": 1.4966983795166016,
      "learning_rate": 0.00046236698378038026,
      "loss": 0.144,
      "step": 115
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 1.0943349599838257,
      "learning_rate": 0.0004617269555450715,
      "loss": 0.1128,
      "step": 116
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.0247364044189453,
      "learning_rate": 0.00046108198137550377,
      "loss": 0.1139,
      "step": 117
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 1.2753956317901611,
      "learning_rate": 0.00046043207633825114,
      "loss": 0.1339,
      "step": 118
    },
    {
      "epoch": 4.576923076923077,
      "grad_norm": 1.0919808149337769,
      "learning_rate": 0.0004597772556150723,
      "loss": 0.1073,
      "step": 119
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 1.3801610469818115,
      "learning_rate": 0.0004591175345025567,
      "loss": 0.1671,
      "step": 120
    },
    {
      "epoch": 4.653846153846154,
      "grad_norm": 0.8968355655670166,
      "learning_rate": 0.0004584529284117662,
      "loss": 0.0919,
      "step": 121
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 1.0625264644622803,
      "learning_rate": 0.0004577834528678757,
      "loss": 0.0952,
      "step": 122
    },
    {
      "epoch": 4.730769230769231,
      "grad_norm": 1.0689988136291504,
      "learning_rate": 0.0004571091235098106,
      "loss": 0.1267,
      "step": 123
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 1.4789851903915405,
      "learning_rate": 0.00045642995608988104,
      "loss": 0.18,
      "step": 124
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 1.4940247535705566,
      "learning_rate": 0.0004557459664734141,
      "loss": 0.2084,
      "step": 125
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 1.1183314323425293,
      "learning_rate": 0.00045505717063838323,
      "loss": 0.1274,
      "step": 126
    },
    {
      "epoch": 4.884615384615385,
      "grad_norm": 1.338548183441162,
      "learning_rate": 0.000454363584675035,
      "loss": 0.1584,
      "step": 127
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 1.5318282842636108,
      "learning_rate": 0.0004536652247855133,
      "loss": 0.3125,
      "step": 128
    },
    {
      "epoch": 4.961538461538462,
      "grad_norm": 1.175580620765686,
      "learning_rate": 0.0004529621072834805,
      "loss": 0.1174,
      "step": 129
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.3294296264648438,
      "learning_rate": 0.0004522542485937369,
      "loss": 0.0954,
      "step": 130
    },
    {
      "epoch": 5.038461538461538,
      "grad_norm": 0.9828715920448303,
      "learning_rate": 0.0004515416652518366,
      "loss": 0.0832,
      "step": 131
    },
    {
      "epoch": 5.076923076923077,
      "grad_norm": 0.8045016527175903,
      "learning_rate": 0.00045082437390370157,
      "loss": 0.0614,
      "step": 132
    },
    {
      "epoch": 5.115384615384615,
      "grad_norm": 0.7405532002449036,
      "learning_rate": 0.00045010239130523254,
      "loss": 0.0505,
      "step": 133
    },
    {
      "epoch": 5.153846153846154,
      "grad_norm": 0.6523577570915222,
      "learning_rate": 0.00044937573432191763,
      "loss": 0.0512,
      "step": 134
    },
    {
      "epoch": 5.1923076923076925,
      "grad_norm": 0.8465009927749634,
      "learning_rate": 0.0004486444199284386,
      "loss": 0.0491,
      "step": 135
    },
    {
      "epoch": 5.230769230769231,
      "grad_norm": 1.0395716428756714,
      "learning_rate": 0.00044790846520827393,
      "loss": 0.079,
      "step": 136
    },
    {
      "epoch": 5.269230769230769,
      "grad_norm": 0.7989003658294678,
      "learning_rate": 0.00044716788735330016,
      "loss": 0.0648,
      "step": 137
    },
    {
      "epoch": 5.3076923076923075,
      "grad_norm": 0.8520196676254272,
      "learning_rate": 0.00044642270366339006,
      "loss": 0.0679,
      "step": 138
    },
    {
      "epoch": 5.346153846153846,
      "grad_norm": 0.672469973564148,
      "learning_rate": 0.0004456729315460084,
      "loss": 0.0455,
      "step": 139
    },
    {
      "epoch": 5.384615384615385,
      "grad_norm": 0.9173368215560913,
      "learning_rate": 0.00044491858851580557,
      "loss": 0.0603,
      "step": 140
    },
    {
      "epoch": 5.423076923076923,
      "grad_norm": 0.8288770914077759,
      "learning_rate": 0.00044415969219420843,
      "loss": 0.0776,
      "step": 141
    },
    {
      "epoch": 5.461538461538462,
      "grad_norm": 0.780058741569519,
      "learning_rate": 0.00044339626030900827,
      "loss": 0.0554,
      "step": 142
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.6203885674476624,
      "learning_rate": 0.0004426283106939473,
      "loss": 0.05,
      "step": 143
    },
    {
      "epoch": 5.538461538461538,
      "grad_norm": 1.111887812614441,
      "learning_rate": 0.0004418558612883016,
      "loss": 0.08,
      "step": 144
    },
    {
      "epoch": 5.576923076923077,
      "grad_norm": 0.7797343134880066,
      "learning_rate": 0.00044107893013646207,
      "loss": 0.0519,
      "step": 145
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 1.4579318761825562,
      "learning_rate": 0.0004402975353875134,
      "loss": 0.0825,
      "step": 146
    },
    {
      "epoch": 5.653846153846154,
      "grad_norm": 1.2866065502166748,
      "learning_rate": 0.00043951169529480926,
      "loss": 0.0843,
      "step": 147
    },
    {
      "epoch": 5.6923076923076925,
      "grad_norm": 1.3344041109085083,
      "learning_rate": 0.0004387214282155469,
      "loss": 0.1179,
      "step": 148
    },
    {
      "epoch": 5.730769230769231,
      "grad_norm": 1.046292781829834,
      "learning_rate": 0.00043792675261033744,
      "loss": 0.0897,
      "step": 149
    },
    {
      "epoch": 5.769230769230769,
      "grad_norm": 1.0912147760391235,
      "learning_rate": 0.00043712768704277526,
      "loss": 0.0846,
      "step": 150
    },
    {
      "epoch": 5.8076923076923075,
      "grad_norm": 1.2637107372283936,
      "learning_rate": 0.000436324250179004,
      "loss": 0.1239,
      "step": 151
    },
    {
      "epoch": 5.846153846153846,
      "grad_norm": 0.9955964088439941,
      "learning_rate": 0.00043551646078728056,
      "loss": 0.072,
      "step": 152
    },
    {
      "epoch": 5.884615384615385,
      "grad_norm": 0.8877971172332764,
      "learning_rate": 0.00043470433773753683,
      "loss": 0.0619,
      "step": 153
    },
    {
      "epoch": 5.923076923076923,
      "grad_norm": 1.032663106918335,
      "learning_rate": 0.0004338879000009388,
      "loss": 0.0769,
      "step": 154
    },
    {
      "epoch": 5.961538461538462,
      "grad_norm": 0.8673059344291687,
      "learning_rate": 0.0004330671666494434,
      "loss": 0.0709,
      "step": 155
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.4756181240081787,
      "learning_rate": 0.00043224215685535287,
      "loss": 0.0827,
      "step": 156
    },
    {
      "epoch": 6.038461538461538,
      "grad_norm": 0.4847783148288727,
      "learning_rate": 0.0004314128898908672,
      "loss": 0.0233,
      "step": 157
    },
    {
      "epoch": 6.076923076923077,
      "grad_norm": 0.7224952578544617,
      "learning_rate": 0.0004305793851276335,
      "loss": 0.0588,
      "step": 158
    },
    {
      "epoch": 6.115384615384615,
      "grad_norm": 0.6980536580085754,
      "learning_rate": 0.0004297416620362938,
      "loss": 0.0409,
      "step": 159
    },
    {
      "epoch": 6.153846153846154,
      "grad_norm": 0.6684603095054626,
      "learning_rate": 0.0004288997401860303,
      "loss": 0.0302,
      "step": 160
    },
    {
      "epoch": 6.1923076923076925,
      "grad_norm": 0.5870895385742188,
      "learning_rate": 0.00042805363924410775,
      "loss": 0.0306,
      "step": 161
    },
    {
      "epoch": 6.230769230769231,
      "grad_norm": 0.47617727518081665,
      "learning_rate": 0.00042720337897541455,
      "loss": 0.0263,
      "step": 162
    },
    {
      "epoch": 6.269230769230769,
      "grad_norm": 0.553171694278717,
      "learning_rate": 0.00042634897924200075,
      "loss": 0.0264,
      "step": 163
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 0.9770476222038269,
      "learning_rate": 0.0004254904600026143,
      "loss": 0.0497,
      "step": 164
    },
    {
      "epoch": 6.346153846153846,
      "grad_norm": 0.7357045412063599,
      "learning_rate": 0.0004246278413122343,
      "loss": 0.0398,
      "step": 165
    },
    {
      "epoch": 6.384615384615385,
      "grad_norm": 0.8679937124252319,
      "learning_rate": 0.0004237611433216032,
      "loss": 0.046,
      "step": 166
    },
    {
      "epoch": 6.423076923076923,
      "grad_norm": 0.7077845931053162,
      "learning_rate": 0.0004228903862767558,
      "loss": 0.0349,
      "step": 167
    },
    {
      "epoch": 6.461538461538462,
      "grad_norm": 0.9489689469337463,
      "learning_rate": 0.00042201559051854604,
      "loss": 0.043,
      "step": 168
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.6276716589927673,
      "learning_rate": 0.0004211367764821722,
      "loss": 0.0319,
      "step": 169
    },
    {
      "epoch": 6.538461538461538,
      "grad_norm": 0.6960490942001343,
      "learning_rate": 0.0004202539646966993,
      "loss": 0.0411,
      "step": 170
    },
    {
      "epoch": 6.576923076923077,
      "grad_norm": 0.7673655152320862,
      "learning_rate": 0.00041936717578457973,
      "loss": 0.0761,
      "step": 171
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 0.7423715591430664,
      "learning_rate": 0.00041847643046117146,
      "loss": 0.043,
      "step": 172
    },
    {
      "epoch": 6.653846153846154,
      "grad_norm": 0.4103946387767792,
      "learning_rate": 0.00041758174953425393,
      "loss": 0.0209,
      "step": 173
    },
    {
      "epoch": 6.6923076923076925,
      "grad_norm": 0.5695048570632935,
      "learning_rate": 0.0004166831539035423,
      "loss": 0.0312,
      "step": 174
    },
    {
      "epoch": 6.730769230769231,
      "grad_norm": 0.7171339392662048,
      "learning_rate": 0.0004157806645601988,
      "loss": 0.0405,
      "step": 175
    },
    {
      "epoch": 6.769230769230769,
      "grad_norm": 0.8081769347190857,
      "learning_rate": 0.0004148743025863431,
      "loss": 0.0584,
      "step": 176
    },
    {
      "epoch": 6.8076923076923075,
      "grad_norm": 0.9434121251106262,
      "learning_rate": 0.00041396408915455907,
      "loss": 0.062,
      "step": 177
    },
    {
      "epoch": 6.846153846153846,
      "grad_norm": 0.7859284281730652,
      "learning_rate": 0.0004130500455274005,
      "loss": 0.0493,
      "step": 178
    },
    {
      "epoch": 6.884615384615385,
      "grad_norm": 0.7478713393211365,
      "learning_rate": 0.00041213219305689453,
      "loss": 0.0408,
      "step": 179
    },
    {
      "epoch": 6.923076923076923,
      "grad_norm": 0.9012162685394287,
      "learning_rate": 0.00041121055318404264,
      "loss": 0.0454,
      "step": 180
    },
    {
      "epoch": 6.961538461538462,
      "grad_norm": 1.1445400714874268,
      "learning_rate": 0.00041028514743832,
      "loss": 0.079,
      "step": 181
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.7863852977752686,
      "learning_rate": 0.00040935599743717243,
      "loss": 0.0462,
      "step": 182
    },
    {
      "epoch": 7.038461538461538,
      "grad_norm": 0.6232797503471375,
      "learning_rate": 0.0004084231248855113,
      "loss": 0.0194,
      "step": 183
    },
    {
      "epoch": 7.076923076923077,
      "grad_norm": 0.4233980178833008,
      "learning_rate": 0.00040748655157520677,
      "loss": 0.0158,
      "step": 184
    },
    {
      "epoch": 7.115384615384615,
      "grad_norm": 0.37682417035102844,
      "learning_rate": 0.0004065462993845784,
      "loss": 0.0141,
      "step": 185
    },
    {
      "epoch": 7.153846153846154,
      "grad_norm": 0.3430037200450897,
      "learning_rate": 0.0004056023902778846,
      "loss": 0.0125,
      "step": 186
    },
    {
      "epoch": 7.1923076923076925,
      "grad_norm": 0.4129306375980377,
      "learning_rate": 0.00040465484630480884,
      "loss": 0.022,
      "step": 187
    },
    {
      "epoch": 7.230769230769231,
      "grad_norm": 0.45412373542785645,
      "learning_rate": 0.0004037036895999453,
      "loss": 0.0175,
      "step": 188
    },
    {
      "epoch": 7.269230769230769,
      "grad_norm": 0.4543265402317047,
      "learning_rate": 0.0004027489423822811,
      "loss": 0.0192,
      "step": 189
    },
    {
      "epoch": 7.3076923076923075,
      "grad_norm": 0.4609105885028839,
      "learning_rate": 0.0004017906269546778,
      "loss": 0.0216,
      "step": 190
    },
    {
      "epoch": 7.346153846153846,
      "grad_norm": 0.6399229764938354,
      "learning_rate": 0.00040082876570335027,
      "loss": 0.0265,
      "step": 191
    },
    {
      "epoch": 7.384615384615385,
      "grad_norm": 0.5859528183937073,
      "learning_rate": 0.0003998633810973435,
      "loss": 0.0231,
      "step": 192
    },
    {
      "epoch": 7.423076923076923,
      "grad_norm": 0.36250025033950806,
      "learning_rate": 0.00039889449568800817,
      "loss": 0.0146,
      "step": 193
    },
    {
      "epoch": 7.461538461538462,
      "grad_norm": 0.5789588093757629,
      "learning_rate": 0.0003979221321084734,
      "loss": 0.0222,
      "step": 194
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.8089443445205688,
      "learning_rate": 0.0003969463130731183,
      "loss": 0.0409,
      "step": 195
    },
    {
      "epoch": 7.538461538461538,
      "grad_norm": 0.42091187834739685,
      "learning_rate": 0.0003959670613770414,
      "loss": 0.0158,
      "step": 196
    },
    {
      "epoch": 7.576923076923077,
      "grad_norm": 0.46365946531295776,
      "learning_rate": 0.0003949843998955279,
      "loss": 0.017,
      "step": 197
    },
    {
      "epoch": 7.615384615384615,
      "grad_norm": 0.5364140868186951,
      "learning_rate": 0.00039399835158351567,
      "loss": 0.0192,
      "step": 198
    },
    {
      "epoch": 7.653846153846154,
      "grad_norm": 0.35953226685523987,
      "learning_rate": 0.0003930089394750586,
      "loss": 0.0199,
      "step": 199
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 0.6717000603675842,
      "learning_rate": 0.00039201618668278893,
      "loss": 0.0205,
      "step": 200
    },
    {
      "epoch": 7.730769230769231,
      "grad_norm": 0.5545645952224731,
      "learning_rate": 0.0003910201163973771,
      "loss": 0.0234,
      "step": 201
    },
    {
      "epoch": 7.769230769230769,
      "grad_norm": 0.6162391304969788,
      "learning_rate": 0.00039002075188699004,
      "loss": 0.0314,
      "step": 202
    },
    {
      "epoch": 7.8076923076923075,
      "grad_norm": 0.5168357491493225,
      "learning_rate": 0.00038901811649674756,
      "loss": 0.0179,
      "step": 203
    },
    {
      "epoch": 7.846153846153846,
      "grad_norm": 0.5697202086448669,
      "learning_rate": 0.0003880122336481774,
      "loss": 0.0242,
      "step": 204
    },
    {
      "epoch": 7.884615384615385,
      "grad_norm": 0.45538392663002014,
      "learning_rate": 0.0003870031268386676,
      "loss": 0.0233,
      "step": 205
    },
    {
      "epoch": 7.923076923076923,
      "grad_norm": 0.6793610453605652,
      "learning_rate": 0.00038599081964091765,
      "loss": 0.0387,
      "step": 206
    },
    {
      "epoch": 7.961538461538462,
      "grad_norm": 0.4277096390724182,
      "learning_rate": 0.0003849753357023885,
      "loss": 0.0114,
      "step": 207
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.7968383431434631,
      "learning_rate": 0.00038395669874474915,
      "loss": 0.0352,
      "step": 208
    },
    {
      "epoch": 8.038461538461538,
      "grad_norm": 0.29702186584472656,
      "learning_rate": 0.00038293493256332336,
      "loss": 0.0115,
      "step": 209
    },
    {
      "epoch": 8.076923076923077,
      "grad_norm": 0.2740010917186737,
      "learning_rate": 0.00038191006102653317,
      "loss": 0.0104,
      "step": 210
    },
    {
      "epoch": 8.115384615384615,
      "grad_norm": 0.2985740900039673,
      "learning_rate": 0.00038088210807534184,
      "loss": 0.0092,
      "step": 211
    },
    {
      "epoch": 8.153846153846153,
      "grad_norm": 0.47116923332214355,
      "learning_rate": 0.0003798510977226943,
      "loss": 0.0107,
      "step": 212
    },
    {
      "epoch": 8.192307692307692,
      "grad_norm": 0.9017041325569153,
      "learning_rate": 0.00037881705405295616,
      "loss": 0.034,
      "step": 213
    },
    {
      "epoch": 8.23076923076923,
      "grad_norm": 0.2524830996990204,
      "learning_rate": 0.00037778000122135134,
      "loss": 0.0072,
      "step": 214
    },
    {
      "epoch": 8.26923076923077,
      "grad_norm": 0.15467317402362823,
      "learning_rate": 0.00037673996345339764,
      "loss": 0.0061,
      "step": 215
    },
    {
      "epoch": 8.307692307692308,
      "grad_norm": 0.27213793992996216,
      "learning_rate": 0.0003756969650443408,
      "loss": 0.0093,
      "step": 216
    },
    {
      "epoch": 8.346153846153847,
      "grad_norm": 0.4386776387691498,
      "learning_rate": 0.00037465103035858716,
      "loss": 0.0206,
      "step": 217
    },
    {
      "epoch": 8.384615384615385,
      "grad_norm": 0.2340622842311859,
      "learning_rate": 0.00037360218382913427,
      "loss": 0.0086,
      "step": 218
    },
    {
      "epoch": 8.423076923076923,
      "grad_norm": 0.4017670452594757,
      "learning_rate": 0.00037255044995700024,
      "loss": 0.0121,
      "step": 219
    },
    {
      "epoch": 8.461538461538462,
      "grad_norm": 0.2935009300708771,
      "learning_rate": 0.0003714958533106515,
      "loss": 0.0083,
      "step": 220
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.28822004795074463,
      "learning_rate": 0.0003704384185254288,
      "loss": 0.0089,
      "step": 221
    },
    {
      "epoch": 8.538461538461538,
      "grad_norm": 0.2429276555776596,
      "learning_rate": 0.0003693781703029716,
      "loss": 0.0095,
      "step": 222
    },
    {
      "epoch": 8.576923076923077,
      "grad_norm": 0.14026997983455658,
      "learning_rate": 0.0003683151334106413,
      "loss": 0.0049,
      "step": 223
    },
    {
      "epoch": 8.615384615384615,
      "grad_norm": 0.3351542055606842,
      "learning_rate": 0.0003672493326809422,
      "loss": 0.0136,
      "step": 224
    },
    {
      "epoch": 8.653846153846153,
      "grad_norm": 0.37046536803245544,
      "learning_rate": 0.00036618079301094214,
      "loss": 0.0155,
      "step": 225
    },
    {
      "epoch": 8.692307692307692,
      "grad_norm": 0.34605881571769714,
      "learning_rate": 0.00036510953936169036,
      "loss": 0.0111,
      "step": 226
    },
    {
      "epoch": 8.73076923076923,
      "grad_norm": 0.1729838252067566,
      "learning_rate": 0.00036403559675763454,
      "loss": 0.0108,
      "step": 227
    },
    {
      "epoch": 8.76923076923077,
      "grad_norm": 0.26249751448631287,
      "learning_rate": 0.0003629589902860363,
      "loss": 0.0098,
      "step": 228
    },
    {
      "epoch": 8.807692307692308,
      "grad_norm": 0.2669985890388489,
      "learning_rate": 0.00036187974509638494,
      "loss": 0.011,
      "step": 229
    },
    {
      "epoch": 8.846153846153847,
      "grad_norm": 0.20964299142360687,
      "learning_rate": 0.00036079788639981037,
      "loss": 0.0064,
      "step": 230
    },
    {
      "epoch": 8.884615384615385,
      "grad_norm": 0.2765675485134125,
      "learning_rate": 0.0003597134394684937,
      "loss": 0.0073,
      "step": 231
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 0.5596491694450378,
      "learning_rate": 0.0003586264296350775,
      "loss": 0.0161,
      "step": 232
    },
    {
      "epoch": 8.961538461538462,
      "grad_norm": 0.37692487239837646,
      "learning_rate": 0.000357536882292073,
      "loss": 0.0147,
      "step": 233
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.26119884848594666,
      "learning_rate": 0.0003564448228912682,
      "loss": 0.0084,
      "step": 234
    }
  ],
  "logging_steps": 1,
  "max_steps": 650,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.064488495745843e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
