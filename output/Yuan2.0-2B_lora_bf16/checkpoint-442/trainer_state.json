{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 17.0,
  "eval_steps": 500,
  "global_step": 442,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 3.062490701675415,
      "learning_rate": 0.0004999970800043822,
      "loss": 2.3277,
      "step": 1
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 0.8911314010620117,
      "learning_rate": 0.0004999883200857396,
      "loss": 2.2973,
      "step": 2
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 0.875841498374939,
      "learning_rate": 0.000499973720448704,
      "loss": 2.2863,
      "step": 3
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 0.8000545501708984,
      "learning_rate": 0.0004999532814343218,
      "loss": 2.0042,
      "step": 4
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 1.0447192192077637,
      "learning_rate": 0.0004999270035200482,
      "loss": 2.1547,
      "step": 5
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 0.8461388945579529,
      "learning_rate": 0.0004998948873197343,
      "loss": 1.7036,
      "step": 6
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 0.953872799873352,
      "learning_rate": 0.000499856933583613,
      "loss": 2.1819,
      "step": 7
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 0.8914564847946167,
      "learning_rate": 0.0004998131431982825,
      "loss": 1.9024,
      "step": 8
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 1.0784956216812134,
      "learning_rate": 0.0004997635171866848,
      "loss": 2.5142,
      "step": 9
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 0.8521708846092224,
      "learning_rate": 0.0004997080567080817,
      "loss": 2.3314,
      "step": 10
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 0.8128117918968201,
      "learning_rate": 0.000499646763058028,
      "loss": 1.9351,
      "step": 11
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 0.6701896786689758,
      "learning_rate": 0.000499579637668341,
      "loss": 1.713,
      "step": 12
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9138104915618896,
      "learning_rate": 0.0004995066821070679,
      "loss": 2.039,
      "step": 13
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 0.9113458395004272,
      "learning_rate": 0.0004994278980784478,
      "loss": 1.4375,
      "step": 14
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 0.9478543400764465,
      "learning_rate": 0.0004993432874228728,
      "loss": 1.9903,
      "step": 15
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.7804792523384094,
      "learning_rate": 0.0004992528521168449,
      "loss": 1.4921,
      "step": 16
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 1.0119937658309937,
      "learning_rate": 0.0004991565942729299,
      "loss": 1.8789,
      "step": 17
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 0.8728306293487549,
      "learning_rate": 0.0004990545161397072,
      "loss": 2.1199,
      "step": 18
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 0.7694759964942932,
      "learning_rate": 0.0004989466201017188,
      "loss": 1.676,
      "step": 19
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 0.7507703900337219,
      "learning_rate": 0.0004988329086794121,
      "loss": 1.4953,
      "step": 20
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 0.7191950082778931,
      "learning_rate": 0.0004987133845290822,
      "loss": 1.6628,
      "step": 21
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 0.6677369475364685,
      "learning_rate": 0.000498588050442809,
      "loss": 1.7452,
      "step": 22
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 0.8734827637672424,
      "learning_rate": 0.0004984569093483921,
      "loss": 1.8428,
      "step": 23
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.7459938526153564,
      "learning_rate": 0.0004983199643092832,
      "loss": 1.5512,
      "step": 24
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 0.9081287980079651,
      "learning_rate": 0.0004981772185245135,
      "loss": 1.7426,
      "step": 25
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6891407370567322,
      "learning_rate": 0.0004980286753286195,
      "loss": 1.3501,
      "step": 26
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 0.8247389197349548,
      "learning_rate": 0.000497874338191565,
      "loss": 2.0243,
      "step": 27
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 0.570045530796051,
      "learning_rate": 0.0004977142107186602,
      "loss": 0.9634,
      "step": 28
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 0.7795639634132385,
      "learning_rate": 0.0004975482966504772,
      "loss": 1.8785,
      "step": 29
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 0.9852104783058167,
      "learning_rate": 0.0004973765998627628,
      "loss": 1.6077,
      "step": 30
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 0.6977394819259644,
      "learning_rate": 0.000497199124366348,
      "loss": 1.2593,
      "step": 31
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 0.9683637022972107,
      "learning_rate": 0.0004970158743070541,
      "loss": 1.492,
      "step": 32
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 0.86052405834198,
      "learning_rate": 0.0004968268539655963,
      "loss": 1.4458,
      "step": 33
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.8665276765823364,
      "learning_rate": 0.0004966320677574827,
      "loss": 1.4314,
      "step": 34
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 0.8010600805282593,
      "learning_rate": 0.0004964315202329127,
      "loss": 1.8197,
      "step": 35
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 1.1157814264297485,
      "learning_rate": 0.0004962252160766693,
      "loss": 1.9557,
      "step": 36
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 0.8435461521148682,
      "learning_rate": 0.0004960131601080103,
      "loss": 1.3016,
      "step": 37
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 0.901960551738739,
      "learning_rate": 0.0004957953572805558,
      "loss": 1.6526,
      "step": 38
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.778903603553772,
      "learning_rate": 0.0004955718126821722,
      "loss": 1.0099,
      "step": 39
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 0.9405866861343384,
      "learning_rate": 0.0004953425315348534,
      "loss": 1.3401,
      "step": 40
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 0.888658881187439,
      "learning_rate": 0.0004951075191945989,
      "loss": 1.1657,
      "step": 41
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.829555094242096,
      "learning_rate": 0.000494866781151289,
      "loss": 1.0982,
      "step": 42
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 0.8553903102874756,
      "learning_rate": 0.0004946203230285557,
      "loss": 0.9545,
      "step": 43
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 0.9111601114273071,
      "learning_rate": 0.0004943681505836522,
      "loss": 1.2767,
      "step": 44
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 1.0618258714675903,
      "learning_rate": 0.0004941102697073181,
      "loss": 1.4246,
      "step": 45
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 0.8991016149520874,
      "learning_rate": 0.0004938466864236413,
      "loss": 0.9724,
      "step": 46
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 1.0322668552398682,
      "learning_rate": 0.0004935774068899184,
      "loss": 1.4172,
      "step": 47
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 0.9276526570320129,
      "learning_rate": 0.0004933024373965097,
      "loss": 1.1873,
      "step": 48
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 0.8406540155410767,
      "learning_rate": 0.0004930217843666929,
      "loss": 1.1714,
      "step": 49
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.9606499671936035,
      "learning_rate": 0.000492735454356513,
      "loss": 1.0526,
      "step": 50
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 0.9613040685653687,
      "learning_rate": 0.000492443454054629,
      "loss": 1.0692,
      "step": 51
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1655267477035522,
      "learning_rate": 0.0004921457902821578,
      "loss": 1.4181,
      "step": 52
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 0.9515112042427063,
      "learning_rate": 0.0004918424699925146,
      "loss": 0.9476,
      "step": 53
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 1.0348467826843262,
      "learning_rate": 0.0004915335002712506,
      "loss": 1.2883,
      "step": 54
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 1.1461851596832275,
      "learning_rate": 0.0004912188883358879,
      "loss": 1.0417,
      "step": 55
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 1.3489856719970703,
      "learning_rate": 0.0004908986415357501,
      "loss": 0.8954,
      "step": 56
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 1.0444974899291992,
      "learning_rate": 0.0004905727673517914,
      "loss": 0.7511,
      "step": 57
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 1.2593330144882202,
      "learning_rate": 0.0004902412733964212,
      "loss": 0.7648,
      "step": 58
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 1.1966091394424438,
      "learning_rate": 0.0004899041674133265,
      "loss": 0.7959,
      "step": 59
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 1.2979122400283813,
      "learning_rate": 0.0004895614572772916,
      "loss": 0.7444,
      "step": 60
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 1.0687201023101807,
      "learning_rate": 0.0004892131509940129,
      "loss": 0.6503,
      "step": 61
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 1.1869425773620605,
      "learning_rate": 0.0004888592566999134,
      "loss": 0.5829,
      "step": 62
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 1.3409245014190674,
      "learning_rate": 0.0004884997826619511,
      "loss": 0.7492,
      "step": 63
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 2.0238184928894043,
      "learning_rate": 0.000488134737277427,
      "loss": 1.2244,
      "step": 64
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.66524076461792,
      "learning_rate": 0.0004877641290737884,
      "loss": 0.7633,
      "step": 65
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 1.128322958946228,
      "learning_rate": 0.00048738796670843004,
      "loss": 0.5971,
      "step": 66
    },
    {
      "epoch": 2.5769230769230766,
      "grad_norm": 1.5947389602661133,
      "learning_rate": 0.0004870062589684916,
      "loss": 1.0266,
      "step": 67
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 1.2906712293624878,
      "learning_rate": 0.00048661901477065246,
      "loss": 0.6898,
      "step": 68
    },
    {
      "epoch": 2.6538461538461537,
      "grad_norm": 1.4268180131912231,
      "learning_rate": 0.0004862262431609235,
      "loss": 0.76,
      "step": 69
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 1.1682125329971313,
      "learning_rate": 0.00048582795331443573,
      "loss": 0.697,
      "step": 70
    },
    {
      "epoch": 2.730769230769231,
      "grad_norm": 1.2468115091323853,
      "learning_rate": 0.0004854241545352262,
      "loss": 0.6049,
      "step": 71
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 1.6212148666381836,
      "learning_rate": 0.00048501485625601995,
      "loss": 1.0165,
      "step": 72
    },
    {
      "epoch": 2.8076923076923075,
      "grad_norm": 1.2294979095458984,
      "learning_rate": 0.00048460006803801057,
      "loss": 0.7809,
      "step": 73
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 1.5046968460083008,
      "learning_rate": 0.00048417979957063626,
      "loss": 0.9687,
      "step": 74
    },
    {
      "epoch": 2.8846153846153846,
      "grad_norm": 1.2751010656356812,
      "learning_rate": 0.00048375406067135377,
      "loss": 0.5697,
      "step": 75
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 1.437254786491394,
      "learning_rate": 0.0004833228612854087,
      "loss": 0.9493,
      "step": 76
    },
    {
      "epoch": 2.9615384615384617,
      "grad_norm": 1.149757742881775,
      "learning_rate": 0.00048288621148560374,
      "loss": 0.6825,
      "step": 77
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.5624420642852783,
      "learning_rate": 0.00048244412147206283,
      "loss": 0.6551,
      "step": 78
    },
    {
      "epoch": 3.0384615384615383,
      "grad_norm": 1.3018054962158203,
      "learning_rate": 0.0004819966015719933,
      "loss": 0.3913,
      "step": 79
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 1.5454065799713135,
      "learning_rate": 0.00048154366223944413,
      "loss": 0.5638,
      "step": 80
    },
    {
      "epoch": 3.1153846153846154,
      "grad_norm": 0.9651458859443665,
      "learning_rate": 0.0004810853140550624,
      "loss": 0.2996,
      "step": 81
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 1.918836236000061,
      "learning_rate": 0.0004806215677258456,
      "loss": 0.4722,
      "step": 82
    },
    {
      "epoch": 3.1923076923076925,
      "grad_norm": 3.3249425888061523,
      "learning_rate": 0.0004801524340848917,
      "loss": 0.5774,
      "step": 83
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 1.6645357608795166,
      "learning_rate": 0.0004796779240911461,
      "loss": 0.309,
      "step": 84
    },
    {
      "epoch": 3.269230769230769,
      "grad_norm": 1.801596760749817,
      "learning_rate": 0.00047919804882914566,
      "loss": 0.3199,
      "step": 85
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 1.2561839818954468,
      "learning_rate": 0.0004787128195087596,
      "loss": 0.1961,
      "step": 86
    },
    {
      "epoch": 3.3461538461538463,
      "grad_norm": 1.696256399154663,
      "learning_rate": 0.0004782222474649279,
      "loss": 0.3921,
      "step": 87
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 1.709801197052002,
      "learning_rate": 0.00047772634415739625,
      "loss": 0.3262,
      "step": 88
    },
    {
      "epoch": 3.423076923076923,
      "grad_norm": 1.4217872619628906,
      "learning_rate": 0.00047722512117044863,
      "loss": 0.264,
      "step": 89
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 1.4318925142288208,
      "learning_rate": 0.00047671859021263636,
      "loss": 0.3307,
      "step": 90
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.3107478618621826,
      "learning_rate": 0.0004762067631165049,
      "loss": 0.3257,
      "step": 91
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 1.5463006496429443,
      "learning_rate": 0.00047568965183831727,
      "loss": 0.4498,
      "step": 92
    },
    {
      "epoch": 3.5769230769230766,
      "grad_norm": 1.6494580507278442,
      "learning_rate": 0.0004751672684577747,
      "loss": 0.3226,
      "step": 93
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 1.367875337600708,
      "learning_rate": 0.00047463962517773473,
      "loss": 0.2571,
      "step": 94
    },
    {
      "epoch": 3.6538461538461537,
      "grad_norm": 1.487399697303772,
      "learning_rate": 0.0004741067343239259,
      "loss": 0.3475,
      "step": 95
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 1.2220972776412964,
      "learning_rate": 0.0004735686083446599,
      "loss": 0.274,
      "step": 96
    },
    {
      "epoch": 3.730769230769231,
      "grad_norm": 1.1666843891143799,
      "learning_rate": 0.00047302525981054067,
      "loss": 0.3128,
      "step": 97
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 1.6472101211547852,
      "learning_rate": 0.000472476701414171,
      "loss": 0.5345,
      "step": 98
    },
    {
      "epoch": 3.8076923076923075,
      "grad_norm": 1.2984520196914673,
      "learning_rate": 0.0004719229459698556,
      "loss": 0.2678,
      "step": 99
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 1.9854828119277954,
      "learning_rate": 0.00047136400641330245,
      "loss": 0.7538,
      "step": 100
    },
    {
      "epoch": 3.8846153846153846,
      "grad_norm": 1.6915010213851929,
      "learning_rate": 0.00047079989580132,
      "loss": 0.5318,
      "step": 101
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 1.5479490756988525,
      "learning_rate": 0.00047023062731151217,
      "loss": 0.3552,
      "step": 102
    },
    {
      "epoch": 3.9615384615384617,
      "grad_norm": 0.9466544389724731,
      "learning_rate": 0.00046965621424197116,
      "loss": 0.2351,
      "step": 103
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.953060269355774,
      "learning_rate": 0.0004690766700109659,
      "loss": 0.582,
      "step": 104
    },
    {
      "epoch": 4.038461538461538,
      "grad_norm": 0.9763156771659851,
      "learning_rate": 0.0004684920081566295,
      "loss": 0.1589,
      "step": 105
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 1.1878637075424194,
      "learning_rate": 0.00046790224233664236,
      "loss": 0.1381,
      "step": 106
    },
    {
      "epoch": 4.115384615384615,
      "grad_norm": 1.0606249570846558,
      "learning_rate": 0.00046730738632791327,
      "loss": 0.1387,
      "step": 107
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 1.2649343013763428,
      "learning_rate": 0.0004667074540262577,
      "loss": 0.1089,
      "step": 108
    },
    {
      "epoch": 4.1923076923076925,
      "grad_norm": 1.0589369535446167,
      "learning_rate": 0.00046610245944607334,
      "loss": 0.0815,
      "step": 109
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 2.0785624980926514,
      "learning_rate": 0.0004654924167200123,
      "loss": 0.1566,
      "step": 110
    },
    {
      "epoch": 4.269230769230769,
      "grad_norm": 1.4229745864868164,
      "learning_rate": 0.00046487734009865127,
      "loss": 0.1276,
      "step": 111
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 1.3154816627502441,
      "learning_rate": 0.0004642572439501586,
      "loss": 0.1527,
      "step": 112
    },
    {
      "epoch": 4.346153846153846,
      "grad_norm": 1.0467482805252075,
      "learning_rate": 0.00046363214275995854,
      "loss": 0.0854,
      "step": 113
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 0.9731992483139038,
      "learning_rate": 0.00046300205113039295,
      "loss": 0.1006,
      "step": 114
    },
    {
      "epoch": 4.423076923076923,
      "grad_norm": 1.4966983795166016,
      "learning_rate": 0.00046236698378038026,
      "loss": 0.144,
      "step": 115
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 1.0943349599838257,
      "learning_rate": 0.0004617269555450715,
      "loss": 0.1128,
      "step": 116
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.0247364044189453,
      "learning_rate": 0.00046108198137550377,
      "loss": 0.1139,
      "step": 117
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 1.2753956317901611,
      "learning_rate": 0.00046043207633825114,
      "loss": 0.1339,
      "step": 118
    },
    {
      "epoch": 4.576923076923077,
      "grad_norm": 1.0919808149337769,
      "learning_rate": 0.0004597772556150723,
      "loss": 0.1073,
      "step": 119
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 1.3801610469818115,
      "learning_rate": 0.0004591175345025567,
      "loss": 0.1671,
      "step": 120
    },
    {
      "epoch": 4.653846153846154,
      "grad_norm": 0.8968355655670166,
      "learning_rate": 0.0004584529284117662,
      "loss": 0.0919,
      "step": 121
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 1.0625264644622803,
      "learning_rate": 0.0004577834528678757,
      "loss": 0.0952,
      "step": 122
    },
    {
      "epoch": 4.730769230769231,
      "grad_norm": 1.0689988136291504,
      "learning_rate": 0.0004571091235098106,
      "loss": 0.1267,
      "step": 123
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 1.4789851903915405,
      "learning_rate": 0.00045642995608988104,
      "loss": 0.18,
      "step": 124
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 1.4940247535705566,
      "learning_rate": 0.0004557459664734141,
      "loss": 0.2084,
      "step": 125
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 1.1183314323425293,
      "learning_rate": 0.00045505717063838323,
      "loss": 0.1274,
      "step": 126
    },
    {
      "epoch": 4.884615384615385,
      "grad_norm": 1.338548183441162,
      "learning_rate": 0.000454363584675035,
      "loss": 0.1584,
      "step": 127
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 1.5318282842636108,
      "learning_rate": 0.0004536652247855133,
      "loss": 0.3125,
      "step": 128
    },
    {
      "epoch": 4.961538461538462,
      "grad_norm": 1.175580620765686,
      "learning_rate": 0.0004529621072834805,
      "loss": 0.1174,
      "step": 129
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.3294296264648438,
      "learning_rate": 0.0004522542485937369,
      "loss": 0.0954,
      "step": 130
    },
    {
      "epoch": 5.038461538461538,
      "grad_norm": 0.9828715920448303,
      "learning_rate": 0.0004515416652518366,
      "loss": 0.0832,
      "step": 131
    },
    {
      "epoch": 5.076923076923077,
      "grad_norm": 0.8045016527175903,
      "learning_rate": 0.00045082437390370157,
      "loss": 0.0614,
      "step": 132
    },
    {
      "epoch": 5.115384615384615,
      "grad_norm": 0.7405532002449036,
      "learning_rate": 0.00045010239130523254,
      "loss": 0.0505,
      "step": 133
    },
    {
      "epoch": 5.153846153846154,
      "grad_norm": 0.6523577570915222,
      "learning_rate": 0.00044937573432191763,
      "loss": 0.0512,
      "step": 134
    },
    {
      "epoch": 5.1923076923076925,
      "grad_norm": 0.8465009927749634,
      "learning_rate": 0.0004486444199284386,
      "loss": 0.0491,
      "step": 135
    },
    {
      "epoch": 5.230769230769231,
      "grad_norm": 1.0395716428756714,
      "learning_rate": 0.00044790846520827393,
      "loss": 0.079,
      "step": 136
    },
    {
      "epoch": 5.269230769230769,
      "grad_norm": 0.7989003658294678,
      "learning_rate": 0.00044716788735330016,
      "loss": 0.0648,
      "step": 137
    },
    {
      "epoch": 5.3076923076923075,
      "grad_norm": 0.8520196676254272,
      "learning_rate": 0.00044642270366339006,
      "loss": 0.0679,
      "step": 138
    },
    {
      "epoch": 5.346153846153846,
      "grad_norm": 0.672469973564148,
      "learning_rate": 0.0004456729315460084,
      "loss": 0.0455,
      "step": 139
    },
    {
      "epoch": 5.384615384615385,
      "grad_norm": 0.9173368215560913,
      "learning_rate": 0.00044491858851580557,
      "loss": 0.0603,
      "step": 140
    },
    {
      "epoch": 5.423076923076923,
      "grad_norm": 0.8288770914077759,
      "learning_rate": 0.00044415969219420843,
      "loss": 0.0776,
      "step": 141
    },
    {
      "epoch": 5.461538461538462,
      "grad_norm": 0.780058741569519,
      "learning_rate": 0.00044339626030900827,
      "loss": 0.0554,
      "step": 142
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.6203885674476624,
      "learning_rate": 0.0004426283106939473,
      "loss": 0.05,
      "step": 143
    },
    {
      "epoch": 5.538461538461538,
      "grad_norm": 1.111887812614441,
      "learning_rate": 0.0004418558612883016,
      "loss": 0.08,
      "step": 144
    },
    {
      "epoch": 5.576923076923077,
      "grad_norm": 0.7797343134880066,
      "learning_rate": 0.00044107893013646207,
      "loss": 0.0519,
      "step": 145
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 1.4579318761825562,
      "learning_rate": 0.0004402975353875134,
      "loss": 0.0825,
      "step": 146
    },
    {
      "epoch": 5.653846153846154,
      "grad_norm": 1.2866065502166748,
      "learning_rate": 0.00043951169529480926,
      "loss": 0.0843,
      "step": 147
    },
    {
      "epoch": 5.6923076923076925,
      "grad_norm": 1.3344041109085083,
      "learning_rate": 0.0004387214282155469,
      "loss": 0.1179,
      "step": 148
    },
    {
      "epoch": 5.730769230769231,
      "grad_norm": 1.046292781829834,
      "learning_rate": 0.00043792675261033744,
      "loss": 0.0897,
      "step": 149
    },
    {
      "epoch": 5.769230769230769,
      "grad_norm": 1.0912147760391235,
      "learning_rate": 0.00043712768704277526,
      "loss": 0.0846,
      "step": 150
    },
    {
      "epoch": 5.8076923076923075,
      "grad_norm": 1.2637107372283936,
      "learning_rate": 0.000436324250179004,
      "loss": 0.1239,
      "step": 151
    },
    {
      "epoch": 5.846153846153846,
      "grad_norm": 0.9955964088439941,
      "learning_rate": 0.00043551646078728056,
      "loss": 0.072,
      "step": 152
    },
    {
      "epoch": 5.884615384615385,
      "grad_norm": 0.8877971172332764,
      "learning_rate": 0.00043470433773753683,
      "loss": 0.0619,
      "step": 153
    },
    {
      "epoch": 5.923076923076923,
      "grad_norm": 1.032663106918335,
      "learning_rate": 0.0004338879000009388,
      "loss": 0.0769,
      "step": 154
    },
    {
      "epoch": 5.961538461538462,
      "grad_norm": 0.8673059344291687,
      "learning_rate": 0.0004330671666494434,
      "loss": 0.0709,
      "step": 155
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.4756181240081787,
      "learning_rate": 0.00043224215685535287,
      "loss": 0.0827,
      "step": 156
    },
    {
      "epoch": 6.038461538461538,
      "grad_norm": 0.4847783148288727,
      "learning_rate": 0.0004314128898908672,
      "loss": 0.0233,
      "step": 157
    },
    {
      "epoch": 6.076923076923077,
      "grad_norm": 0.7224952578544617,
      "learning_rate": 0.0004305793851276335,
      "loss": 0.0588,
      "step": 158
    },
    {
      "epoch": 6.115384615384615,
      "grad_norm": 0.6980536580085754,
      "learning_rate": 0.0004297416620362938,
      "loss": 0.0409,
      "step": 159
    },
    {
      "epoch": 6.153846153846154,
      "grad_norm": 0.6684603095054626,
      "learning_rate": 0.0004288997401860303,
      "loss": 0.0302,
      "step": 160
    },
    {
      "epoch": 6.1923076923076925,
      "grad_norm": 0.5870895385742188,
      "learning_rate": 0.00042805363924410775,
      "loss": 0.0306,
      "step": 161
    },
    {
      "epoch": 6.230769230769231,
      "grad_norm": 0.47617727518081665,
      "learning_rate": 0.00042720337897541455,
      "loss": 0.0263,
      "step": 162
    },
    {
      "epoch": 6.269230769230769,
      "grad_norm": 0.553171694278717,
      "learning_rate": 0.00042634897924200075,
      "loss": 0.0264,
      "step": 163
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 0.9770476222038269,
      "learning_rate": 0.0004254904600026143,
      "loss": 0.0497,
      "step": 164
    },
    {
      "epoch": 6.346153846153846,
      "grad_norm": 0.7357045412063599,
      "learning_rate": 0.0004246278413122343,
      "loss": 0.0398,
      "step": 165
    },
    {
      "epoch": 6.384615384615385,
      "grad_norm": 0.8679937124252319,
      "learning_rate": 0.0004237611433216032,
      "loss": 0.046,
      "step": 166
    },
    {
      "epoch": 6.423076923076923,
      "grad_norm": 0.7077845931053162,
      "learning_rate": 0.0004228903862767558,
      "loss": 0.0349,
      "step": 167
    },
    {
      "epoch": 6.461538461538462,
      "grad_norm": 0.9489689469337463,
      "learning_rate": 0.00042201559051854604,
      "loss": 0.043,
      "step": 168
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.6276716589927673,
      "learning_rate": 0.0004211367764821722,
      "loss": 0.0319,
      "step": 169
    },
    {
      "epoch": 6.538461538461538,
      "grad_norm": 0.6960490942001343,
      "learning_rate": 0.0004202539646966993,
      "loss": 0.0411,
      "step": 170
    },
    {
      "epoch": 6.576923076923077,
      "grad_norm": 0.7673655152320862,
      "learning_rate": 0.00041936717578457973,
      "loss": 0.0761,
      "step": 171
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 0.7423715591430664,
      "learning_rate": 0.00041847643046117146,
      "loss": 0.043,
      "step": 172
    },
    {
      "epoch": 6.653846153846154,
      "grad_norm": 0.4103946387767792,
      "learning_rate": 0.00041758174953425393,
      "loss": 0.0209,
      "step": 173
    },
    {
      "epoch": 6.6923076923076925,
      "grad_norm": 0.5695048570632935,
      "learning_rate": 0.0004166831539035423,
      "loss": 0.0312,
      "step": 174
    },
    {
      "epoch": 6.730769230769231,
      "grad_norm": 0.7171339392662048,
      "learning_rate": 0.0004157806645601988,
      "loss": 0.0405,
      "step": 175
    },
    {
      "epoch": 6.769230769230769,
      "grad_norm": 0.8081769347190857,
      "learning_rate": 0.0004148743025863431,
      "loss": 0.0584,
      "step": 176
    },
    {
      "epoch": 6.8076923076923075,
      "grad_norm": 0.9434121251106262,
      "learning_rate": 0.00041396408915455907,
      "loss": 0.062,
      "step": 177
    },
    {
      "epoch": 6.846153846153846,
      "grad_norm": 0.7859284281730652,
      "learning_rate": 0.0004130500455274005,
      "loss": 0.0493,
      "step": 178
    },
    {
      "epoch": 6.884615384615385,
      "grad_norm": 0.7478713393211365,
      "learning_rate": 0.00041213219305689453,
      "loss": 0.0408,
      "step": 179
    },
    {
      "epoch": 6.923076923076923,
      "grad_norm": 0.9012162685394287,
      "learning_rate": 0.00041121055318404264,
      "loss": 0.0454,
      "step": 180
    },
    {
      "epoch": 6.961538461538462,
      "grad_norm": 1.1445400714874268,
      "learning_rate": 0.00041028514743832,
      "loss": 0.079,
      "step": 181
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.7863852977752686,
      "learning_rate": 0.00040935599743717243,
      "loss": 0.0462,
      "step": 182
    },
    {
      "epoch": 7.038461538461538,
      "grad_norm": 0.6232797503471375,
      "learning_rate": 0.0004084231248855113,
      "loss": 0.0194,
      "step": 183
    },
    {
      "epoch": 7.076923076923077,
      "grad_norm": 0.4233980178833008,
      "learning_rate": 0.00040748655157520677,
      "loss": 0.0158,
      "step": 184
    },
    {
      "epoch": 7.115384615384615,
      "grad_norm": 0.37682417035102844,
      "learning_rate": 0.0004065462993845784,
      "loss": 0.0141,
      "step": 185
    },
    {
      "epoch": 7.153846153846154,
      "grad_norm": 0.3430037200450897,
      "learning_rate": 0.0004056023902778846,
      "loss": 0.0125,
      "step": 186
    },
    {
      "epoch": 7.1923076923076925,
      "grad_norm": 0.4129306375980377,
      "learning_rate": 0.00040465484630480884,
      "loss": 0.022,
      "step": 187
    },
    {
      "epoch": 7.230769230769231,
      "grad_norm": 0.45412373542785645,
      "learning_rate": 0.0004037036895999453,
      "loss": 0.0175,
      "step": 188
    },
    {
      "epoch": 7.269230769230769,
      "grad_norm": 0.4543265402317047,
      "learning_rate": 0.0004027489423822811,
      "loss": 0.0192,
      "step": 189
    },
    {
      "epoch": 7.3076923076923075,
      "grad_norm": 0.4609105885028839,
      "learning_rate": 0.0004017906269546778,
      "loss": 0.0216,
      "step": 190
    },
    {
      "epoch": 7.346153846153846,
      "grad_norm": 0.6399229764938354,
      "learning_rate": 0.00040082876570335027,
      "loss": 0.0265,
      "step": 191
    },
    {
      "epoch": 7.384615384615385,
      "grad_norm": 0.5859528183937073,
      "learning_rate": 0.0003998633810973435,
      "loss": 0.0231,
      "step": 192
    },
    {
      "epoch": 7.423076923076923,
      "grad_norm": 0.36250025033950806,
      "learning_rate": 0.00039889449568800817,
      "loss": 0.0146,
      "step": 193
    },
    {
      "epoch": 7.461538461538462,
      "grad_norm": 0.5789588093757629,
      "learning_rate": 0.0003979221321084734,
      "loss": 0.0222,
      "step": 194
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.8089443445205688,
      "learning_rate": 0.0003969463130731183,
      "loss": 0.0409,
      "step": 195
    },
    {
      "epoch": 7.538461538461538,
      "grad_norm": 0.42091187834739685,
      "learning_rate": 0.0003959670613770414,
      "loss": 0.0158,
      "step": 196
    },
    {
      "epoch": 7.576923076923077,
      "grad_norm": 0.46365946531295776,
      "learning_rate": 0.0003949843998955279,
      "loss": 0.017,
      "step": 197
    },
    {
      "epoch": 7.615384615384615,
      "grad_norm": 0.5364140868186951,
      "learning_rate": 0.00039399835158351567,
      "loss": 0.0192,
      "step": 198
    },
    {
      "epoch": 7.653846153846154,
      "grad_norm": 0.35953226685523987,
      "learning_rate": 0.0003930089394750586,
      "loss": 0.0199,
      "step": 199
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 0.6717000603675842,
      "learning_rate": 0.00039201618668278893,
      "loss": 0.0205,
      "step": 200
    },
    {
      "epoch": 7.730769230769231,
      "grad_norm": 0.5545645952224731,
      "learning_rate": 0.0003910201163973771,
      "loss": 0.0234,
      "step": 201
    },
    {
      "epoch": 7.769230769230769,
      "grad_norm": 0.6162391304969788,
      "learning_rate": 0.00039002075188699004,
      "loss": 0.0314,
      "step": 202
    },
    {
      "epoch": 7.8076923076923075,
      "grad_norm": 0.5168357491493225,
      "learning_rate": 0.00038901811649674756,
      "loss": 0.0179,
      "step": 203
    },
    {
      "epoch": 7.846153846153846,
      "grad_norm": 0.5697202086448669,
      "learning_rate": 0.0003880122336481774,
      "loss": 0.0242,
      "step": 204
    },
    {
      "epoch": 7.884615384615385,
      "grad_norm": 0.45538392663002014,
      "learning_rate": 0.0003870031268386676,
      "loss": 0.0233,
      "step": 205
    },
    {
      "epoch": 7.923076923076923,
      "grad_norm": 0.6793610453605652,
      "learning_rate": 0.00038599081964091765,
      "loss": 0.0387,
      "step": 206
    },
    {
      "epoch": 7.961538461538462,
      "grad_norm": 0.4277096390724182,
      "learning_rate": 0.0003849753357023885,
      "loss": 0.0114,
      "step": 207
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.7968383431434631,
      "learning_rate": 0.00038395669874474915,
      "loss": 0.0352,
      "step": 208
    },
    {
      "epoch": 8.038461538461538,
      "grad_norm": 0.29702186584472656,
      "learning_rate": 0.00038293493256332336,
      "loss": 0.0115,
      "step": 209
    },
    {
      "epoch": 8.076923076923077,
      "grad_norm": 0.2740010917186737,
      "learning_rate": 0.00038191006102653317,
      "loss": 0.0104,
      "step": 210
    },
    {
      "epoch": 8.115384615384615,
      "grad_norm": 0.2985740900039673,
      "learning_rate": 0.00038088210807534184,
      "loss": 0.0092,
      "step": 211
    },
    {
      "epoch": 8.153846153846153,
      "grad_norm": 0.47116923332214355,
      "learning_rate": 0.0003798510977226943,
      "loss": 0.0107,
      "step": 212
    },
    {
      "epoch": 8.192307692307692,
      "grad_norm": 0.9017041325569153,
      "learning_rate": 0.00037881705405295616,
      "loss": 0.034,
      "step": 213
    },
    {
      "epoch": 8.23076923076923,
      "grad_norm": 0.2524830996990204,
      "learning_rate": 0.00037778000122135134,
      "loss": 0.0072,
      "step": 214
    },
    {
      "epoch": 8.26923076923077,
      "grad_norm": 0.15467317402362823,
      "learning_rate": 0.00037673996345339764,
      "loss": 0.0061,
      "step": 215
    },
    {
      "epoch": 8.307692307692308,
      "grad_norm": 0.27213793992996216,
      "learning_rate": 0.0003756969650443408,
      "loss": 0.0093,
      "step": 216
    },
    {
      "epoch": 8.346153846153847,
      "grad_norm": 0.4386776387691498,
      "learning_rate": 0.00037465103035858716,
      "loss": 0.0206,
      "step": 217
    },
    {
      "epoch": 8.384615384615385,
      "grad_norm": 0.2340622842311859,
      "learning_rate": 0.00037360218382913427,
      "loss": 0.0086,
      "step": 218
    },
    {
      "epoch": 8.423076923076923,
      "grad_norm": 0.4017670452594757,
      "learning_rate": 0.00037255044995700024,
      "loss": 0.0121,
      "step": 219
    },
    {
      "epoch": 8.461538461538462,
      "grad_norm": 0.2935009300708771,
      "learning_rate": 0.0003714958533106515,
      "loss": 0.0083,
      "step": 220
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.28822004795074463,
      "learning_rate": 0.0003704384185254288,
      "loss": 0.0089,
      "step": 221
    },
    {
      "epoch": 8.538461538461538,
      "grad_norm": 0.2429276555776596,
      "learning_rate": 0.0003693781703029716,
      "loss": 0.0095,
      "step": 222
    },
    {
      "epoch": 8.576923076923077,
      "grad_norm": 0.14026997983455658,
      "learning_rate": 0.0003683151334106413,
      "loss": 0.0049,
      "step": 223
    },
    {
      "epoch": 8.615384615384615,
      "grad_norm": 0.3351542055606842,
      "learning_rate": 0.0003672493326809422,
      "loss": 0.0136,
      "step": 224
    },
    {
      "epoch": 8.653846153846153,
      "grad_norm": 0.37046536803245544,
      "learning_rate": 0.00036618079301094214,
      "loss": 0.0155,
      "step": 225
    },
    {
      "epoch": 8.692307692307692,
      "grad_norm": 0.34605881571769714,
      "learning_rate": 0.00036510953936169036,
      "loss": 0.0111,
      "step": 226
    },
    {
      "epoch": 8.73076923076923,
      "grad_norm": 0.1729838252067566,
      "learning_rate": 0.00036403559675763454,
      "loss": 0.0108,
      "step": 227
    },
    {
      "epoch": 8.76923076923077,
      "grad_norm": 0.26249751448631287,
      "learning_rate": 0.0003629589902860363,
      "loss": 0.0098,
      "step": 228
    },
    {
      "epoch": 8.807692307692308,
      "grad_norm": 0.2669985890388489,
      "learning_rate": 0.00036187974509638494,
      "loss": 0.011,
      "step": 229
    },
    {
      "epoch": 8.846153846153847,
      "grad_norm": 0.20964299142360687,
      "learning_rate": 0.00036079788639981037,
      "loss": 0.0064,
      "step": 230
    },
    {
      "epoch": 8.884615384615385,
      "grad_norm": 0.2765675485134125,
      "learning_rate": 0.0003597134394684937,
      "loss": 0.0073,
      "step": 231
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 0.5596491694450378,
      "learning_rate": 0.0003586264296350775,
      "loss": 0.0161,
      "step": 232
    },
    {
      "epoch": 8.961538461538462,
      "grad_norm": 0.37692487239837646,
      "learning_rate": 0.000357536882292073,
      "loss": 0.0147,
      "step": 233
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.26119884848594666,
      "learning_rate": 0.0003564448228912682,
      "loss": 0.0084,
      "step": 234
    },
    {
      "epoch": 9.038461538461538,
      "grad_norm": 0.17918023467063904,
      "learning_rate": 0.0003553502769431323,
      "loss": 0.0041,
      "step": 235
    },
    {
      "epoch": 9.076923076923077,
      "grad_norm": 0.22038382291793823,
      "learning_rate": 0.00035425327001622033,
      "loss": 0.0037,
      "step": 236
    },
    {
      "epoch": 9.115384615384615,
      "grad_norm": 0.3328598439693451,
      "learning_rate": 0.0003531538277365756,
      "loss": 0.0064,
      "step": 237
    },
    {
      "epoch": 9.153846153846153,
      "grad_norm": 0.2700952887535095,
      "learning_rate": 0.00035205197578713125,
      "loss": 0.0064,
      "step": 238
    },
    {
      "epoch": 9.192307692307692,
      "grad_norm": 0.3832571506500244,
      "learning_rate": 0.0003509477399071102,
      "loss": 0.0108,
      "step": 239
    },
    {
      "epoch": 9.23076923076923,
      "grad_norm": 0.25721508264541626,
      "learning_rate": 0.0003498411458914239,
      "loss": 0.0097,
      "step": 240
    },
    {
      "epoch": 9.26923076923077,
      "grad_norm": 0.17877477407455444,
      "learning_rate": 0.00034873221959006974,
      "loss": 0.0052,
      "step": 241
    },
    {
      "epoch": 9.307692307692308,
      "grad_norm": 0.1313476711511612,
      "learning_rate": 0.00034762098690752725,
      "loss": 0.0032,
      "step": 242
    },
    {
      "epoch": 9.346153846153847,
      "grad_norm": 0.22774635255336761,
      "learning_rate": 0.0003465074738021529,
      "loss": 0.0038,
      "step": 243
    },
    {
      "epoch": 9.384615384615385,
      "grad_norm": 0.23797820508480072,
      "learning_rate": 0.0003453917062855738,
      "loss": 0.0108,
      "step": 244
    },
    {
      "epoch": 9.423076923076923,
      "grad_norm": 0.251289039850235,
      "learning_rate": 0.00034427371042208013,
      "loss": 0.0084,
      "step": 245
    },
    {
      "epoch": 9.461538461538462,
      "grad_norm": 0.3001057207584381,
      "learning_rate": 0.00034315351232801596,
      "loss": 0.0068,
      "step": 246
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.17291483283042908,
      "learning_rate": 0.00034203113817116957,
      "loss": 0.0034,
      "step": 247
    },
    {
      "epoch": 9.538461538461538,
      "grad_norm": 0.265180766582489,
      "learning_rate": 0.00034090661417016175,
      "loss": 0.0122,
      "step": 248
    },
    {
      "epoch": 9.576923076923077,
      "grad_norm": 0.36964601278305054,
      "learning_rate": 0.0003397799665938339,
      "loss": 0.0089,
      "step": 249
    },
    {
      "epoch": 9.615384615384615,
      "grad_norm": 0.12313185632228851,
      "learning_rate": 0.0003386512217606339,
      "loss": 0.0038,
      "step": 250
    },
    {
      "epoch": 9.653846153846153,
      "grad_norm": 0.09669851511716843,
      "learning_rate": 0.00033752040603800146,
      "loss": 0.0035,
      "step": 251
    },
    {
      "epoch": 9.692307692307692,
      "grad_norm": 0.24263359606266022,
      "learning_rate": 0.00033638754584175217,
      "loss": 0.0073,
      "step": 252
    },
    {
      "epoch": 9.73076923076923,
      "grad_norm": 0.23982639610767365,
      "learning_rate": 0.0003352526676354606,
      "loss": 0.0049,
      "step": 253
    },
    {
      "epoch": 9.76923076923077,
      "grad_norm": 0.16372229158878326,
      "learning_rate": 0.0003341157979298418,
      "loss": 0.0046,
      "step": 254
    },
    {
      "epoch": 9.807692307692308,
      "grad_norm": 0.406444251537323,
      "learning_rate": 0.00033297696328213214,
      "loss": 0.0099,
      "step": 255
    },
    {
      "epoch": 9.846153846153847,
      "grad_norm": 0.08675894886255264,
      "learning_rate": 0.0003318361902954692,
      "loss": 0.0037,
      "step": 256
    },
    {
      "epoch": 9.884615384615385,
      "grad_norm": 0.18850620090961456,
      "learning_rate": 0.0003306935056182699,
      "loss": 0.0081,
      "step": 257
    },
    {
      "epoch": 9.923076923076923,
      "grad_norm": 0.2821676731109619,
      "learning_rate": 0.0003295489359436083,
      "loss": 0.0107,
      "step": 258
    },
    {
      "epoch": 9.961538461538462,
      "grad_norm": 0.3102869987487793,
      "learning_rate": 0.00032840250800859184,
      "loss": 0.0084,
      "step": 259
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.10087905079126358,
      "learning_rate": 0.00032725424859373687,
      "loss": 0.0032,
      "step": 260
    },
    {
      "epoch": 10.038461538461538,
      "grad_norm": 0.28858694434165955,
      "learning_rate": 0.0003261041845223431,
      "loss": 0.0123,
      "step": 261
    },
    {
      "epoch": 10.076923076923077,
      "grad_norm": 0.04692879319190979,
      "learning_rate": 0.0003249523426598669,
      "loss": 0.0024,
      "step": 262
    },
    {
      "epoch": 10.115384615384615,
      "grad_norm": 0.033618465065956116,
      "learning_rate": 0.0003237987499132937,
      "loss": 0.0027,
      "step": 263
    },
    {
      "epoch": 10.153846153846153,
      "grad_norm": 0.06967198103666306,
      "learning_rate": 0.0003226434332305098,
      "loss": 0.0024,
      "step": 264
    },
    {
      "epoch": 10.192307692307692,
      "grad_norm": 0.2215038239955902,
      "learning_rate": 0.0003214864195996723,
      "loss": 0.0067,
      "step": 265
    },
    {
      "epoch": 10.23076923076923,
      "grad_norm": 0.1466016322374344,
      "learning_rate": 0.0003203277360485791,
      "loss": 0.0023,
      "step": 266
    },
    {
      "epoch": 10.26923076923077,
      "grad_norm": 0.18904002010822296,
      "learning_rate": 0.00031916740964403736,
      "loss": 0.0073,
      "step": 267
    },
    {
      "epoch": 10.307692307692308,
      "grad_norm": 0.04597315937280655,
      "learning_rate": 0.00031800546749123106,
      "loss": 0.0023,
      "step": 268
    },
    {
      "epoch": 10.346153846153847,
      "grad_norm": 0.07717345654964447,
      "learning_rate": 0.00031684193673308823,
      "loss": 0.0027,
      "step": 269
    },
    {
      "epoch": 10.384615384615385,
      "grad_norm": 0.1726492941379547,
      "learning_rate": 0.0003156768445496467,
      "loss": 0.0024,
      "step": 270
    },
    {
      "epoch": 10.423076923076923,
      "grad_norm": 0.033306851983070374,
      "learning_rate": 0.000314510218157419,
      "loss": 0.0017,
      "step": 271
    },
    {
      "epoch": 10.461538461538462,
      "grad_norm": 0.12315402179956436,
      "learning_rate": 0.00031334208480875657,
      "loss": 0.0028,
      "step": 272
    },
    {
      "epoch": 10.5,
      "grad_norm": 0.24913036823272705,
      "learning_rate": 0.0003121724717912138,
      "loss": 0.0034,
      "step": 273
    },
    {
      "epoch": 10.538461538461538,
      "grad_norm": 0.18468834459781647,
      "learning_rate": 0.00031100140642690937,
      "loss": 0.0029,
      "step": 274
    },
    {
      "epoch": 10.576923076923077,
      "grad_norm": 0.12130914628505707,
      "learning_rate": 0.00030982891607188946,
      "loss": 0.0023,
      "step": 275
    },
    {
      "epoch": 10.615384615384615,
      "grad_norm": 0.04584846273064613,
      "learning_rate": 0.0003086550281154875,
      "loss": 0.0023,
      "step": 276
    },
    {
      "epoch": 10.653846153846153,
      "grad_norm": 0.08467083424329758,
      "learning_rate": 0.00030747976997968514,
      "loss": 0.0024,
      "step": 277
    },
    {
      "epoch": 10.692307692307692,
      "grad_norm": 0.24103929102420807,
      "learning_rate": 0.00030630316911847114,
      "loss": 0.0043,
      "step": 278
    },
    {
      "epoch": 10.73076923076923,
      "grad_norm": 0.15732631087303162,
      "learning_rate": 0.0003051252530172003,
      "loss": 0.0051,
      "step": 279
    },
    {
      "epoch": 10.76923076923077,
      "grad_norm": 0.11444612592458725,
      "learning_rate": 0.00030394604919195154,
      "loss": 0.0049,
      "step": 280
    },
    {
      "epoch": 10.807692307692308,
      "grad_norm": 0.13682739436626434,
      "learning_rate": 0.0003027655851888849,
      "loss": 0.0027,
      "step": 281
    },
    {
      "epoch": 10.846153846153847,
      "grad_norm": 0.05183376371860504,
      "learning_rate": 0.0003015838885835981,
      "loss": 0.0019,
      "step": 282
    },
    {
      "epoch": 10.884615384615385,
      "grad_norm": 0.05695980787277222,
      "learning_rate": 0.0003004009869804823,
      "loss": 0.0021,
      "step": 283
    },
    {
      "epoch": 10.923076923076923,
      "grad_norm": 0.03040475770831108,
      "learning_rate": 0.00029921690801207757,
      "loss": 0.0019,
      "step": 284
    },
    {
      "epoch": 10.961538461538462,
      "grad_norm": 0.1908087283372879,
      "learning_rate": 0.0002980316793384271,
      "loss": 0.0076,
      "step": 285
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.07629323750734329,
      "learning_rate": 0.0002968453286464312,
      "loss": 0.0022,
      "step": 286
    },
    {
      "epoch": 11.038461538461538,
      "grad_norm": 0.023855717852711678,
      "learning_rate": 0.0002956578836492003,
      "loss": 0.0017,
      "step": 287
    },
    {
      "epoch": 11.076923076923077,
      "grad_norm": 0.022379059344530106,
      "learning_rate": 0.00029446937208540806,
      "loss": 0.0018,
      "step": 288
    },
    {
      "epoch": 11.115384615384615,
      "grad_norm": 0.024620922282338142,
      "learning_rate": 0.00029327982171864286,
      "loss": 0.0019,
      "step": 289
    },
    {
      "epoch": 11.153846153846153,
      "grad_norm": 0.021642455831170082,
      "learning_rate": 0.00029208926033675957,
      "loss": 0.0013,
      "step": 290
    },
    {
      "epoch": 11.192307692307692,
      "grad_norm": 0.023570019751787186,
      "learning_rate": 0.00029089771575123045,
      "loss": 0.0017,
      "step": 291
    },
    {
      "epoch": 11.23076923076923,
      "grad_norm": 0.028419559821486473,
      "learning_rate": 0.00028970521579649517,
      "loss": 0.0017,
      "step": 292
    },
    {
      "epoch": 11.26923076923077,
      "grad_norm": 0.10019297897815704,
      "learning_rate": 0.00028851178832931077,
      "loss": 0.0025,
      "step": 293
    },
    {
      "epoch": 11.307692307692308,
      "grad_norm": 0.028103066608309746,
      "learning_rate": 0.00028731746122810105,
      "loss": 0.007,
      "step": 294
    },
    {
      "epoch": 11.346153846153847,
      "grad_norm": 0.028046514838933945,
      "learning_rate": 0.00028612226239230535,
      "loss": 0.0016,
      "step": 295
    },
    {
      "epoch": 11.384615384615385,
      "grad_norm": 0.017992381006479263,
      "learning_rate": 0.0002849262197417265,
      "loss": 0.0013,
      "step": 296
    },
    {
      "epoch": 11.423076923076923,
      "grad_norm": 0.17008166015148163,
      "learning_rate": 0.00028372936121587897,
      "loss": 0.0023,
      "step": 297
    },
    {
      "epoch": 11.461538461538462,
      "grad_norm": 0.10880404710769653,
      "learning_rate": 0.0002825317147733358,
      "loss": 0.0017,
      "step": 298
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.1083502471446991,
      "learning_rate": 0.00028133330839107606,
      "loss": 0.0015,
      "step": 299
    },
    {
      "epoch": 11.538461538461538,
      "grad_norm": 0.2167605459690094,
      "learning_rate": 0.0002801341700638307,
      "loss": 0.0038,
      "step": 300
    },
    {
      "epoch": 11.576923076923077,
      "grad_norm": 0.04153270274400711,
      "learning_rate": 0.00027893432780342925,
      "loss": 0.0013,
      "step": 301
    },
    {
      "epoch": 11.615384615384615,
      "grad_norm": 0.035593245178461075,
      "learning_rate": 0.00027773380963814456,
      "loss": 0.0016,
      "step": 302
    },
    {
      "epoch": 11.653846153846153,
      "grad_norm": 0.02075670287013054,
      "learning_rate": 0.00027653264361203895,
      "loss": 0.0014,
      "step": 303
    },
    {
      "epoch": 11.692307692307692,
      "grad_norm": 0.17371200025081635,
      "learning_rate": 0.0002753308577843088,
      "loss": 0.003,
      "step": 304
    },
    {
      "epoch": 11.73076923076923,
      "grad_norm": 0.027878884226083755,
      "learning_rate": 0.0002741284802286288,
      "loss": 0.0015,
      "step": 305
    },
    {
      "epoch": 11.76923076923077,
      "grad_norm": 0.016369035467505455,
      "learning_rate": 0.00027292553903249657,
      "loss": 0.0014,
      "step": 306
    },
    {
      "epoch": 11.807692307692308,
      "grad_norm": 0.0844729021191597,
      "learning_rate": 0.0002717220622965762,
      "loss": 0.0021,
      "step": 307
    },
    {
      "epoch": 11.846153846153847,
      "grad_norm": 0.03659965097904205,
      "learning_rate": 0.0002705180781340421,
      "loss": 0.0015,
      "step": 308
    },
    {
      "epoch": 11.884615384615385,
      "grad_norm": 0.05331752076745033,
      "learning_rate": 0.0002693136146699222,
      "loss": 0.0015,
      "step": 309
    },
    {
      "epoch": 11.923076923076923,
      "grad_norm": 0.20728102326393127,
      "learning_rate": 0.00026810870004044063,
      "loss": 0.0069,
      "step": 310
    },
    {
      "epoch": 11.961538461538462,
      "grad_norm": 0.021426986902952194,
      "learning_rate": 0.00026690336239236094,
      "loss": 0.0013,
      "step": 311
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.024609476327896118,
      "learning_rate": 0.0002656976298823284,
      "loss": 0.0019,
      "step": 312
    },
    {
      "epoch": 12.038461538461538,
      "grad_norm": 0.018056482076644897,
      "learning_rate": 0.00026449153067621207,
      "loss": 0.0012,
      "step": 313
    },
    {
      "epoch": 12.076923076923077,
      "grad_norm": 0.05735368654131889,
      "learning_rate": 0.00026328509294844715,
      "loss": 0.0014,
      "step": 314
    },
    {
      "epoch": 12.115384615384615,
      "grad_norm": 0.0193781778216362,
      "learning_rate": 0.0002620783448813768,
      "loss": 0.0012,
      "step": 315
    },
    {
      "epoch": 12.153846153846153,
      "grad_norm": 0.014770682901144028,
      "learning_rate": 0.0002608713146645934,
      "loss": 0.0014,
      "step": 316
    },
    {
      "epoch": 12.192307692307692,
      "grad_norm": 0.0174415223300457,
      "learning_rate": 0.00025966403049428055,
      "loss": 0.0012,
      "step": 317
    },
    {
      "epoch": 12.23076923076923,
      "grad_norm": 0.019042078405618668,
      "learning_rate": 0.00025845652057255415,
      "loss": 0.0013,
      "step": 318
    },
    {
      "epoch": 12.26923076923077,
      "grad_norm": 0.016342584043741226,
      "learning_rate": 0.00025724881310680364,
      "loss": 0.0011,
      "step": 319
    },
    {
      "epoch": 12.307692307692308,
      "grad_norm": 0.018012922257184982,
      "learning_rate": 0.0002560409363090331,
      "loss": 0.0013,
      "step": 320
    },
    {
      "epoch": 12.346153846153847,
      "grad_norm": 0.0452893003821373,
      "learning_rate": 0.0002548329183952021,
      "loss": 0.0016,
      "step": 321
    },
    {
      "epoch": 12.384615384615385,
      "grad_norm": 0.019332168623805046,
      "learning_rate": 0.0002536247875845669,
      "loss": 0.0012,
      "step": 322
    },
    {
      "epoch": 12.423076923076923,
      "grad_norm": 0.03857547789812088,
      "learning_rate": 0.0002524165720990208,
      "loss": 0.0013,
      "step": 323
    },
    {
      "epoch": 12.461538461538462,
      "grad_norm": 0.016463665291666985,
      "learning_rate": 0.0002512083001624351,
      "loss": 0.0011,
      "step": 324
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.0138170151039958,
      "learning_rate": 0.00025,
      "loss": 0.0013,
      "step": 325
    },
    {
      "epoch": 12.538461538461538,
      "grad_norm": 0.03087729774415493,
      "learning_rate": 0.00024879169983756496,
      "loss": 0.0011,
      "step": 326
    },
    {
      "epoch": 12.576923076923077,
      "grad_norm": 0.019965406507253647,
      "learning_rate": 0.00024758342790097925,
      "loss": 0.0009,
      "step": 327
    },
    {
      "epoch": 12.615384615384615,
      "grad_norm": 0.0286894291639328,
      "learning_rate": 0.00024637521241543313,
      "loss": 0.0015,
      "step": 328
    },
    {
      "epoch": 12.653846153846153,
      "grad_norm": 0.0650072991847992,
      "learning_rate": 0.0002451670816047979,
      "loss": 0.0013,
      "step": 329
    },
    {
      "epoch": 12.692307692307692,
      "grad_norm": 0.016688650473952293,
      "learning_rate": 0.000243959063690967,
      "loss": 0.0011,
      "step": 330
    },
    {
      "epoch": 12.73076923076923,
      "grad_norm": 0.01528838649392128,
      "learning_rate": 0.00024275118689319637,
      "loss": 0.001,
      "step": 331
    },
    {
      "epoch": 12.76923076923077,
      "grad_norm": 0.021734025329351425,
      "learning_rate": 0.0002415434794274459,
      "loss": 0.0013,
      "step": 332
    },
    {
      "epoch": 12.807692307692308,
      "grad_norm": 0.01680675707757473,
      "learning_rate": 0.00024033596950571943,
      "loss": 0.0014,
      "step": 333
    },
    {
      "epoch": 12.846153846153847,
      "grad_norm": 0.02254961058497429,
      "learning_rate": 0.00023912868533540663,
      "loss": 0.0011,
      "step": 334
    },
    {
      "epoch": 12.884615384615385,
      "grad_norm": 0.051116943359375,
      "learning_rate": 0.00023792165511862326,
      "loss": 0.0014,
      "step": 335
    },
    {
      "epoch": 12.923076923076923,
      "grad_norm": 0.013467060402035713,
      "learning_rate": 0.00023671490705155286,
      "loss": 0.0011,
      "step": 336
    },
    {
      "epoch": 12.961538461538462,
      "grad_norm": 0.012938879430294037,
      "learning_rate": 0.00023550846932378803,
      "loss": 0.0011,
      "step": 337
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.01784616708755493,
      "learning_rate": 0.00023430237011767165,
      "loss": 0.0061,
      "step": 338
    },
    {
      "epoch": 13.038461538461538,
      "grad_norm": 0.01192130334675312,
      "learning_rate": 0.00023309663760763912,
      "loss": 0.001,
      "step": 339
    },
    {
      "epoch": 13.076923076923077,
      "grad_norm": 0.011686615645885468,
      "learning_rate": 0.00023189129995955944,
      "loss": 0.0013,
      "step": 340
    },
    {
      "epoch": 13.115384615384615,
      "grad_norm": 0.013211634010076523,
      "learning_rate": 0.00023068638533007784,
      "loss": 0.0047,
      "step": 341
    },
    {
      "epoch": 13.153846153846153,
      "grad_norm": 0.01296168752014637,
      "learning_rate": 0.00022948192186595786,
      "loss": 0.0008,
      "step": 342
    },
    {
      "epoch": 13.192307692307692,
      "grad_norm": 0.01358776818960905,
      "learning_rate": 0.00022827793770342384,
      "loss": 0.0011,
      "step": 343
    },
    {
      "epoch": 13.23076923076923,
      "grad_norm": 0.012476706877350807,
      "learning_rate": 0.00022707446096750344,
      "loss": 0.0009,
      "step": 344
    },
    {
      "epoch": 13.26923076923077,
      "grad_norm": 0.014001660980284214,
      "learning_rate": 0.00022587151977137123,
      "loss": 0.001,
      "step": 345
    },
    {
      "epoch": 13.307692307692308,
      "grad_norm": 0.012647518888115883,
      "learning_rate": 0.0002246691422156913,
      "loss": 0.0011,
      "step": 346
    },
    {
      "epoch": 13.346153846153847,
      "grad_norm": 0.01166808046400547,
      "learning_rate": 0.0002234673563879611,
      "loss": 0.001,
      "step": 347
    },
    {
      "epoch": 13.384615384615385,
      "grad_norm": 0.01297325361520052,
      "learning_rate": 0.00022226619036185558,
      "loss": 0.001,
      "step": 348
    },
    {
      "epoch": 13.423076923076923,
      "grad_norm": 0.014868859201669693,
      "learning_rate": 0.0002210656721965708,
      "loss": 0.0009,
      "step": 349
    },
    {
      "epoch": 13.461538461538462,
      "grad_norm": 0.013449447229504585,
      "learning_rate": 0.00021986582993616926,
      "loss": 0.0011,
      "step": 350
    },
    {
      "epoch": 13.5,
      "grad_norm": 0.011232856661081314,
      "learning_rate": 0.00021866669160892392,
      "loss": 0.0011,
      "step": 351
    },
    {
      "epoch": 13.538461538461538,
      "grad_norm": 0.012472668662667274,
      "learning_rate": 0.0002174682852266642,
      "loss": 0.0009,
      "step": 352
    },
    {
      "epoch": 13.576923076923077,
      "grad_norm": 0.013208344578742981,
      "learning_rate": 0.00021627063878412107,
      "loss": 0.0009,
      "step": 353
    },
    {
      "epoch": 13.615384615384615,
      "grad_norm": 0.01099256332963705,
      "learning_rate": 0.00021507378025827352,
      "loss": 0.0009,
      "step": 354
    },
    {
      "epoch": 13.653846153846153,
      "grad_norm": 0.015763748437166214,
      "learning_rate": 0.00021387773760769474,
      "loss": 0.0012,
      "step": 355
    },
    {
      "epoch": 13.692307692307692,
      "grad_norm": 0.011463849805295467,
      "learning_rate": 0.000212682538771899,
      "loss": 0.0011,
      "step": 356
    },
    {
      "epoch": 13.73076923076923,
      "grad_norm": 0.010830794461071491,
      "learning_rate": 0.00021148821167068938,
      "loss": 0.001,
      "step": 357
    },
    {
      "epoch": 13.76923076923077,
      "grad_norm": 0.012864440679550171,
      "learning_rate": 0.0002102947842035049,
      "loss": 0.0011,
      "step": 358
    },
    {
      "epoch": 13.807692307692308,
      "grad_norm": 0.013981832191348076,
      "learning_rate": 0.00020910228424876956,
      "loss": 0.0009,
      "step": 359
    },
    {
      "epoch": 13.846153846153847,
      "grad_norm": 0.012028904631733894,
      "learning_rate": 0.00020791073966324036,
      "loss": 0.001,
      "step": 360
    },
    {
      "epoch": 13.884615384615385,
      "grad_norm": 0.013755442574620247,
      "learning_rate": 0.00020672017828135718,
      "loss": 0.001,
      "step": 361
    },
    {
      "epoch": 13.923076923076923,
      "grad_norm": 0.03441186621785164,
      "learning_rate": 0.00020553062791459192,
      "loss": 0.0014,
      "step": 362
    },
    {
      "epoch": 13.961538461538462,
      "grad_norm": 0.013493201695382595,
      "learning_rate": 0.00020434211635079973,
      "loss": 0.0011,
      "step": 363
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.04450380429625511,
      "learning_rate": 0.0002031546713535688,
      "loss": 0.0011,
      "step": 364
    },
    {
      "epoch": 14.038461538461538,
      "grad_norm": 0.010927977971732616,
      "learning_rate": 0.00020196832066157288,
      "loss": 0.0009,
      "step": 365
    },
    {
      "epoch": 14.076923076923077,
      "grad_norm": 0.020938018336892128,
      "learning_rate": 0.0002007830919879225,
      "loss": 0.0012,
      "step": 366
    },
    {
      "epoch": 14.115384615384615,
      "grad_norm": 0.010361111722886562,
      "learning_rate": 0.00019959901301951777,
      "loss": 0.0009,
      "step": 367
    },
    {
      "epoch": 14.153846153846153,
      "grad_norm": 0.012834138236939907,
      "learning_rate": 0.00019841611141640204,
      "loss": 0.0012,
      "step": 368
    },
    {
      "epoch": 14.192307692307692,
      "grad_norm": 0.01026091631501913,
      "learning_rate": 0.00019723441481111513,
      "loss": 0.0009,
      "step": 369
    },
    {
      "epoch": 14.23076923076923,
      "grad_norm": 0.011521193198859692,
      "learning_rate": 0.0001960539508080485,
      "loss": 0.0011,
      "step": 370
    },
    {
      "epoch": 14.26923076923077,
      "grad_norm": 0.01214651484042406,
      "learning_rate": 0.00019487474698279974,
      "loss": 0.0009,
      "step": 371
    },
    {
      "epoch": 14.307692307692308,
      "grad_norm": 0.011802555061876774,
      "learning_rate": 0.00019369683088152893,
      "loss": 0.0048,
      "step": 372
    },
    {
      "epoch": 14.346153846153847,
      "grad_norm": 0.01229880377650261,
      "learning_rate": 0.00019252023002031487,
      "loss": 0.001,
      "step": 373
    },
    {
      "epoch": 14.384615384615385,
      "grad_norm": 0.01172846183180809,
      "learning_rate": 0.0001913449718845125,
      "loss": 0.001,
      "step": 374
    },
    {
      "epoch": 14.423076923076923,
      "grad_norm": 0.013271639123558998,
      "learning_rate": 0.00019017108392811063,
      "loss": 0.0011,
      "step": 375
    },
    {
      "epoch": 14.461538461538462,
      "grad_norm": 0.013362564146518707,
      "learning_rate": 0.00018899859357309062,
      "loss": 0.0008,
      "step": 376
    },
    {
      "epoch": 14.5,
      "grad_norm": 0.011558427475392818,
      "learning_rate": 0.00018782752820878634,
      "loss": 0.0008,
      "step": 377
    },
    {
      "epoch": 14.538461538461538,
      "grad_norm": 0.008657124824821949,
      "learning_rate": 0.0001866579151912434,
      "loss": 0.0008,
      "step": 378
    },
    {
      "epoch": 14.576923076923077,
      "grad_norm": 0.011387869715690613,
      "learning_rate": 0.0001854897818425811,
      "loss": 0.0008,
      "step": 379
    },
    {
      "epoch": 14.615384615384615,
      "grad_norm": 0.01603233441710472,
      "learning_rate": 0.00018432315545035326,
      "loss": 0.001,
      "step": 380
    },
    {
      "epoch": 14.653846153846153,
      "grad_norm": 0.010385892353951931,
      "learning_rate": 0.00018315806326691175,
      "loss": 0.001,
      "step": 381
    },
    {
      "epoch": 14.692307692307692,
      "grad_norm": 0.01485605351626873,
      "learning_rate": 0.00018199453250876892,
      "loss": 0.001,
      "step": 382
    },
    {
      "epoch": 14.73076923076923,
      "grad_norm": 0.008324967697262764,
      "learning_rate": 0.00018083259035596273,
      "loss": 0.0007,
      "step": 383
    },
    {
      "epoch": 14.76923076923077,
      "grad_norm": 0.01417982205748558,
      "learning_rate": 0.00017967226395142088,
      "loss": 0.0009,
      "step": 384
    },
    {
      "epoch": 14.807692307692308,
      "grad_norm": 0.010116837918758392,
      "learning_rate": 0.00017851358040032772,
      "loss": 0.0009,
      "step": 385
    },
    {
      "epoch": 14.846153846153847,
      "grad_norm": 0.017996665090322495,
      "learning_rate": 0.00017735656676949027,
      "loss": 0.0011,
      "step": 386
    },
    {
      "epoch": 14.884615384615385,
      "grad_norm": 0.009318403899669647,
      "learning_rate": 0.0001762012500867063,
      "loss": 0.0007,
      "step": 387
    },
    {
      "epoch": 14.923076923076923,
      "grad_norm": 0.01293857116252184,
      "learning_rate": 0.00017504765734013323,
      "loss": 0.001,
      "step": 388
    },
    {
      "epoch": 14.961538461538462,
      "grad_norm": 0.012636033818125725,
      "learning_rate": 0.00017389581547765692,
      "loss": 0.0008,
      "step": 389
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.011794737540185452,
      "learning_rate": 0.00017274575140626317,
      "loss": 0.0011,
      "step": 390
    },
    {
      "epoch": 15.038461538461538,
      "grad_norm": 0.009566125459969044,
      "learning_rate": 0.00017159749199140818,
      "loss": 0.0009,
      "step": 391
    },
    {
      "epoch": 15.076923076923077,
      "grad_norm": 0.010792944580316544,
      "learning_rate": 0.00017045106405639174,
      "loss": 0.0008,
      "step": 392
    },
    {
      "epoch": 15.115384615384615,
      "grad_norm": 0.012199386022984982,
      "learning_rate": 0.00016930649438173006,
      "loss": 0.0009,
      "step": 393
    },
    {
      "epoch": 15.153846153846153,
      "grad_norm": 0.011889568530023098,
      "learning_rate": 0.00016816380970453084,
      "loss": 0.001,
      "step": 394
    },
    {
      "epoch": 15.192307692307692,
      "grad_norm": 0.01106369961053133,
      "learning_rate": 0.00016702303671786795,
      "loss": 0.0007,
      "step": 395
    },
    {
      "epoch": 15.23076923076923,
      "grad_norm": 0.011758493259549141,
      "learning_rate": 0.00016588420207015825,
      "loss": 0.001,
      "step": 396
    },
    {
      "epoch": 15.26923076923077,
      "grad_norm": 0.008354452438652515,
      "learning_rate": 0.00016474733236453948,
      "loss": 0.0006,
      "step": 397
    },
    {
      "epoch": 15.307692307692308,
      "grad_norm": 0.011858351528644562,
      "learning_rate": 0.00016361245415824784,
      "loss": 0.0051,
      "step": 398
    },
    {
      "epoch": 15.346153846153847,
      "grad_norm": 0.009583212435245514,
      "learning_rate": 0.0001624795939619986,
      "loss": 0.0008,
      "step": 399
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 0.00961678009480238,
      "learning_rate": 0.00016134877823936609,
      "loss": 0.0009,
      "step": 400
    },
    {
      "epoch": 15.423076923076923,
      "grad_norm": 0.008941740728914738,
      "learning_rate": 0.0001602200334061661,
      "loss": 0.0008,
      "step": 401
    },
    {
      "epoch": 15.461538461538462,
      "grad_norm": 0.00947970524430275,
      "learning_rate": 0.00015909338582983823,
      "loss": 0.0008,
      "step": 402
    },
    {
      "epoch": 15.5,
      "grad_norm": 0.010005438700318336,
      "learning_rate": 0.00015796886182883053,
      "loss": 0.0011,
      "step": 403
    },
    {
      "epoch": 15.538461538461538,
      "grad_norm": 0.012239478528499603,
      "learning_rate": 0.00015684648767198413,
      "loss": 0.0008,
      "step": 404
    },
    {
      "epoch": 15.576923076923077,
      "grad_norm": 0.009932962246239185,
      "learning_rate": 0.0001557262895779199,
      "loss": 0.0008,
      "step": 405
    },
    {
      "epoch": 15.615384615384615,
      "grad_norm": 0.010128061287105083,
      "learning_rate": 0.00015460829371442624,
      "loss": 0.0009,
      "step": 406
    },
    {
      "epoch": 15.653846153846153,
      "grad_norm": 0.010468050837516785,
      "learning_rate": 0.00015349252619784712,
      "loss": 0.0008,
      "step": 407
    },
    {
      "epoch": 15.692307692307692,
      "grad_norm": 0.011395086534321308,
      "learning_rate": 0.0001523790130924728,
      "loss": 0.0009,
      "step": 408
    },
    {
      "epoch": 15.73076923076923,
      "grad_norm": 0.010349364019930363,
      "learning_rate": 0.0001512677804099303,
      "loss": 0.001,
      "step": 409
    },
    {
      "epoch": 15.76923076923077,
      "grad_norm": 0.009677530266344547,
      "learning_rate": 0.00015015885410857616,
      "loss": 0.0007,
      "step": 410
    },
    {
      "epoch": 15.807692307692308,
      "grad_norm": 0.011209123767912388,
      "learning_rate": 0.0001490522600928898,
      "loss": 0.0008,
      "step": 411
    },
    {
      "epoch": 15.846153846153847,
      "grad_norm": 0.015743160620331764,
      "learning_rate": 0.00014794802421286878,
      "loss": 0.0008,
      "step": 412
    },
    {
      "epoch": 15.884615384615385,
      "grad_norm": 0.01011347584426403,
      "learning_rate": 0.0001468461722634244,
      "loss": 0.0008,
      "step": 413
    },
    {
      "epoch": 15.923076923076923,
      "grad_norm": 0.010401842184364796,
      "learning_rate": 0.00014574672998377968,
      "loss": 0.0009,
      "step": 414
    },
    {
      "epoch": 15.961538461538462,
      "grad_norm": 0.008885822258889675,
      "learning_rate": 0.00014464972305686777,
      "loss": 0.001,
      "step": 415
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.010881343856453896,
      "learning_rate": 0.00014355517710873183,
      "loss": 0.0009,
      "step": 416
    },
    {
      "epoch": 16.03846153846154,
      "grad_norm": 0.009850176982581615,
      "learning_rate": 0.00014246311770792702,
      "loss": 0.0009,
      "step": 417
    },
    {
      "epoch": 16.076923076923077,
      "grad_norm": 0.013574006035923958,
      "learning_rate": 0.00014137357036492253,
      "loss": 0.0009,
      "step": 418
    },
    {
      "epoch": 16.115384615384617,
      "grad_norm": 0.009340199641883373,
      "learning_rate": 0.00014028656053150627,
      "loss": 0.0007,
      "step": 419
    },
    {
      "epoch": 16.153846153846153,
      "grad_norm": 0.009699813090264797,
      "learning_rate": 0.0001392021136001897,
      "loss": 0.001,
      "step": 420
    },
    {
      "epoch": 16.192307692307693,
      "grad_norm": 0.009672427549958229,
      "learning_rate": 0.00013812025490361516,
      "loss": 0.0009,
      "step": 421
    },
    {
      "epoch": 16.23076923076923,
      "grad_norm": 0.008195835165679455,
      "learning_rate": 0.00013704100971396378,
      "loss": 0.0009,
      "step": 422
    },
    {
      "epoch": 16.26923076923077,
      "grad_norm": 0.011105095036327839,
      "learning_rate": 0.0001359644032423655,
      "loss": 0.0009,
      "step": 423
    },
    {
      "epoch": 16.307692307692307,
      "grad_norm": 0.007858718745410442,
      "learning_rate": 0.00013489046063830973,
      "loss": 0.0008,
      "step": 424
    },
    {
      "epoch": 16.346153846153847,
      "grad_norm": 0.009411770850419998,
      "learning_rate": 0.00013381920698905787,
      "loss": 0.0007,
      "step": 425
    },
    {
      "epoch": 16.384615384615383,
      "grad_norm": 0.009237631224095821,
      "learning_rate": 0.0001327506673190579,
      "loss": 0.0008,
      "step": 426
    },
    {
      "epoch": 16.423076923076923,
      "grad_norm": 0.012156828306615353,
      "learning_rate": 0.00013168486658935878,
      "loss": 0.0009,
      "step": 427
    },
    {
      "epoch": 16.46153846153846,
      "grad_norm": 0.008662973530590534,
      "learning_rate": 0.0001306218296970284,
      "loss": 0.0006,
      "step": 428
    },
    {
      "epoch": 16.5,
      "grad_norm": 0.00917014665901661,
      "learning_rate": 0.00012956158147457115,
      "loss": 0.0008,
      "step": 429
    },
    {
      "epoch": 16.53846153846154,
      "grad_norm": 0.009921087883412838,
      "learning_rate": 0.00012850414668934848,
      "loss": 0.0008,
      "step": 430
    },
    {
      "epoch": 16.576923076923077,
      "grad_norm": 0.01032035518437624,
      "learning_rate": 0.00012744955004299983,
      "loss": 0.0008,
      "step": 431
    },
    {
      "epoch": 16.615384615384617,
      "grad_norm": 0.012228360399603844,
      "learning_rate": 0.00012639781617086588,
      "loss": 0.0006,
      "step": 432
    },
    {
      "epoch": 16.653846153846153,
      "grad_norm": 0.009945674799382687,
      "learning_rate": 0.0001253489696414129,
      "loss": 0.0008,
      "step": 433
    },
    {
      "epoch": 16.692307692307693,
      "grad_norm": 0.019081443548202515,
      "learning_rate": 0.00012430303495565928,
      "loss": 0.0009,
      "step": 434
    },
    {
      "epoch": 16.73076923076923,
      "grad_norm": 0.008697966113686562,
      "learning_rate": 0.00012326003654660248,
      "loss": 0.0008,
      "step": 435
    },
    {
      "epoch": 16.76923076923077,
      "grad_norm": 0.009668976068496704,
      "learning_rate": 0.0001222199987786487,
      "loss": 0.0007,
      "step": 436
    },
    {
      "epoch": 16.807692307692307,
      "grad_norm": 0.009180460125207901,
      "learning_rate": 0.0001211829459470439,
      "loss": 0.0006,
      "step": 437
    },
    {
      "epoch": 16.846153846153847,
      "grad_norm": 0.009455734863877296,
      "learning_rate": 0.00012014890227730569,
      "loss": 0.0008,
      "step": 438
    },
    {
      "epoch": 16.884615384615383,
      "grad_norm": 0.008257712237536907,
      "learning_rate": 0.00011911789192465816,
      "loss": 0.0008,
      "step": 439
    },
    {
      "epoch": 16.923076923076923,
      "grad_norm": 0.011284034699201584,
      "learning_rate": 0.00011808993897346678,
      "loss": 0.0009,
      "step": 440
    },
    {
      "epoch": 16.96153846153846,
      "grad_norm": 0.010052722878754139,
      "learning_rate": 0.00011706506743667666,
      "loss": 0.0049,
      "step": 441
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.010054193437099457,
      "learning_rate": 0.00011604330125525078,
      "loss": 0.0009,
      "step": 442
    }
  ],
  "logging_steps": 1,
  "max_steps": 650,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.79446906790912e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
